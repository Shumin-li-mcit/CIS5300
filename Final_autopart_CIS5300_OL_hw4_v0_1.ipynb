{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shumin-li-mcit/CIS5300/blob/main/Final_autopart_CIS5300_OL_hw4_v0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 4: Vector Space Models \n",
        "\n",
        "## Due Date: Jun 12th\n",
        "## Total Points: 76 (+ 15 extra credit)\n",
        "- **Overview**: In this assignment you will implement many of the things you learned in [Chapter 6 of the textbook](https://web.stanford.edu/~jurafsky/slp3/6.pdf). If you haven't read it yet, now would be a good time to do that.  We'll wait.  Done?  Great, let's move on. \n",
        "    \n",
        "    We will provide a corpus of Shakespeare plays, which you will use to create a term-document matrix and a term-context matrix. You'll implement a selection of the weighting methods and similarity metrics defined in the textbook. Ultimately, your goal is to use the resulting vectors to measure how similar Shakespeare plays are to each other, and to find words that are used in a similar fashion. All (or almost all) of the code you write will be direct implementations of concepts and equations described in [Chapter 6, sections 6.3-6.7](https://web.stanford.edu/~jurafsky/slp3/6.pdf).\n",
        "\n",
        "    *All difficulties are easy when they are known.*\n",
        "- **Delieverables:** \n",
        "    - Your implementations for the functions in the skeleton code (this notebook)\n",
        "\n",
        "- **Grading**: We will use the auto-grading system called `PennGrader`. To complete the homework assignment, you should implement anything marked with `#TODO` and run the cell with `#PennGrader` note. **There will be no hidden tests in this assignment.** In other words, you will know your score once you finish all the `#TODO` and run all the `#PennGrader` tests!\n",
        "\n",
        "\n",
        "## Recommended Readings\n",
        "\n",
        "- [Vector Semantics](https://web.stanford.edu/~jurafsky/slp3/6.pdf). Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd edition draft)\n",
        "- [From Frequency to Meaning: Vector Space Models of Semantics](https://www.jair.org/media/2934/live-2934-4846-jair.pdf). Peter D. Turney and Patrick Pantel. Journal of Artificial Intelligence Research 2010\n",
        "- [Paraphrasing for Style](http://www.aclweb.org/anthology/C12-1177) Wei Xu, Alan Ritter, Bill Dolan, Ralph Grisman, and Colin Cherry. Coling 2012\n",
        "- [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036) Tobias Schnabel, Igor Labutov, David Mimno, Thorsten Joachims. EMNLP 2015\n",
        "- [Community Evaluation and Exchange of Word Vectors at wordvectors.org.](http://www.aclweb.org/anthology/P14-5004) Manaal Faruqui and Chris Dyer. ACL demos 2014\n",
        "\n",
        "## FAQs\n",
        "\n",
        "- When finding the top 10 similar items for a given target element, should I count the target element?  \n",
        "    *No, do not count the target element.*\n",
        "\n",
        "- How can I represent a character as a vector for calculating similarity?  \n",
        "    *One reasonable way would be to do it much in the same way as for plays. You would just need to write code to segment out each character as the given code did for each play.*\n",
        "\n",
        "- What kind of analysis can I perform on the female and male Shakespearean characters for the report?  \n",
        "    *There can be various ways to go about this. You can look at PCA projections of the vectors representing the characters in the plays to see if the men and women cluster in distinct areas that are demarcated. Or you can look at the similarity scores among women, among men and compare it to the similarity scores between men and men to see if it is significantly different. To account for outliers, you can use averages in this case. Feel free to play around with different vector representations and different similarity measures.*\n",
        "\n",
        "- How can I improve the performance and efficiency of my code?  \n",
        "    *Try to use vectorized code wherever possible instead of using loops. You can refer to this resource on [vectorized code](http://www.cs.cornell.edu/courses/cs1112/2015sp/Exams/exam2/vectorizedCode.pdf).*\n",
        "\n",
        "- How many documents should I consider for reporting similarity scores in the writeup?  \n",
        "    *You need not report similarity scores for every pair of documents. A subset of similarity scores should be sufficient. For instance, you can include the top 10 of one or two documents.*\n",
        "\n",
        "- How can I compute the similarity scores on SimLex999 data set, and compute their correlation with human judgments using Kendall's Tau?  \n",
        "    *You can use simlex data to get a ranking list with your model and calculate the number of concordant and discordant pairs. You can refer to this resource on [Kendall’s Tau](https://www.statisticshowto.datasciencecentral.com/kendalls-tau/).*\n",
        "\n",
        "## To get started, **make a copy** of this colab notebook into your google drive!"
      ],
      "metadata": {
        "id": "6U0N4_Mn8Kt2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDv9qN5c9357"
      },
      "source": [
        "## Setup 1: PennGrader Setup [4 points]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT CHANGE ANYTHING, JUST RUN\n",
        "%%capture\n",
        "!pip install penngrader-client"
      ],
      "metadata": {
        "id": "jpSjR2N19kk4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "FvPA8Z2D9ki_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94b8d14-a021-4b9c-a372-b8fc0a014cd3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting notebook-config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat notebook-config.yaml"
      ],
      "metadata": {
        "id": "xbeXT4Oj9kg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d21d31c-cc1f-4151-8832-2179730bee9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
            "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from penngrader.grader import *\n",
        "\n",
        "## TODO - Start\n",
        "STUDENT_ID = 65847590 # YOUR PENN-ID GOES HERE AS AN INTEGER#\n",
        "## TODO - End\n",
        "\n",
        "SECRET = STUDENT_ID\n",
        "grader = PennGrader('notebook-config.yaml', 'CIS5300_OL_23Su_HW4', STUDENT_ID, SECRET)"
      ],
      "metadata": {
        "id": "OceP0Hr-9kfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329c65c1-df73-4741-8da9-9214d22912f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennGrader initialized with Student ID: 65847590\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the PennGrader is set up correctly\n",
        "# do not chance this cell, see if you get 4/4!\n",
        "name_str = 'Shumin Li'\n",
        "grader.grade(test_case_id = 'name_test', answer = name_str)"
      ],
      "metadata": {
        "id": "k5lAgKaYa-uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09808fc4-2961-45f0-ca92-cf075ad4d74b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHeB9GLB-PO1"
      },
      "source": [
        "## Setup 2: Dataset / Packages\n",
        "- **Run the following cells without changing anything!**\n",
        "- [Loading dataset from huggingface](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html#from-local-files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E6qSD12uEx4T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import subprocess\n",
        "import re\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1usS5D3heEt6MB60KEXoebuU6QLpwiqhJ # https://drive.google.com/file/d/1usS5D3heEt6MB60KEXoebuU6QLpwiqhJ/view?usp=share_link\n",
        "!gdown 1RI7YQIA0WUcMUZRY9Frh-Vcet-js2Cd3 # https://drive.google.com/file/d/1RI7YQIA0WUcMUZRY9Frh-Vcet-js2Cd3/view?usp=share_link\n",
        "!gdown 1c3wLkuSuKkaFIHOQFBK82awMP5H33uBP # https://drive.google.com/file/d/1c3wLkuSuKkaFIHOQFBK82awMP5H33uBP/view?usp=sharing"
      ],
      "metadata": {
        "id": "iosTnYSebMK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6d60b7-9f8c-4047-f49b-36be6d9ccc38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1usS5D3heEt6MB60KEXoebuU6QLpwiqhJ\n",
            "To: /content/play_names.txt\n",
            "100% 550/550 [00:00<00:00, 2.57MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RI7YQIA0WUcMUZRY9Frh-Vcet-js2Cd3\n",
            "To: /content/vocab.txt\n",
            "100% 184k/184k [00:00<00:00, 94.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1c3wLkuSuKkaFIHOQFBK82awMP5H33uBP\n",
            "To: /content/will_play_text.csv\n",
            "100% 10.3M/10.3M [00:00<00:00, 151MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### Helper Functions - DO NOT CHANGE #########\n",
        "def read_in_shakespeare():\n",
        "\t'''Reads in the Shakespeare dataset processesit into a list of tuples.\n",
        "\t\t Also reads in the vocab and play name lists from files.\n",
        "\n",
        "\tEach tuple consists of\n",
        "\ttuple[0]: The name of the play\n",
        "\ttuple[1] A line from the play as a list of tokenized words.\n",
        "\n",
        "\tReturns:\n",
        "\t\ttuples: A list of tuples in the above format.\n",
        "\t\tdocument_names: A list of the plays present in the corpus.\n",
        "\t\tvocab: A list of all tokens in the vocabulary.\n",
        "\t'''\n",
        "\n",
        "\ttuples = []\n",
        "\n",
        "\twith open('will_play_text.csv') as f:\n",
        "\t\tcsv_reader = csv.reader(f, delimiter=';')\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tplay_name = row[1]\n",
        "\t\t\tline = row[5]\n",
        "\t\t\tline_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
        "\t\t\tline_tokens = [token.lower() for token in line_tokens]\n",
        "\n",
        "\t\t\ttuples.append((play_name, line_tokens))\n",
        "\n",
        "\twith open('vocab.txt') as f:\n",
        "\t\tvocab =  [line.strip() for line in f]\n",
        "\n",
        "\twith open('play_names.txt') as f:\n",
        "\t\tdocument_names =  [line.strip() for line in f]\n",
        "\n",
        "\treturn tuples, document_names, vocab\n",
        "\n",
        "def get_row_vector(matrix, row_id):\n",
        "\treturn matrix[row_id, :]\n",
        "\n",
        "def get_column_vector(matrix, col_id):\n",
        "\treturn matrix[:, col_id]\n",
        "######### Helper Functions - DO NOT CHANGE #########"
      ],
      "metadata": {
        "id": "5pmUQQtagSVo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuples, document_names, vocab = read_in_shakespeare()\n",
        "print('tuples: ', tuples[:10], '\\n', 'doc_names: ', document_names[:10], '\\n', 'vocab: ', vocab[:10])"
      ],
      "metadata": {
        "id": "z4V85X7lO4k3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d376d790-bc4c-42b6-898b-3ec663780cd9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuples:  [('Henry IV', ['act', 'i']), ('Henry IV', ['scene', 'i', 'london', 'the', 'palace']), ('Henry IV', ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others']), ('Henry IV', ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care']), ('Henry IV', ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant']), ('Henry IV', ['and', 'breathe', 'short', 'winded', 'accents', 'of', 'new', 'broils']), ('Henry IV', ['to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote']), ('Henry IV', ['no', 'more', 'the', 'thirsty', 'entrance', 'of', 'this', 'soil']), ('Henry IV', ['shall', 'daub', 'her', 'lips', 'with', 'her', 'own', 'children', 's', 'blood']), ('Henry IV', ['nor', 'more', 'shall', 'trenching', 'war', 'channel', 'her', 'fields'])] \n",
            " doc_names:  ['Henry IV', 'Alls well that ends well', 'Loves Labours Lost', 'Taming of the Shrew', 'Antony and Cleopatra', 'Coriolanus', 'Hamlet', 'A Midsummer nights dream', 'Merry Wives of Windsor', 'Romeo and Juliet'] \n",
            " vocab:  ['disliken', 'tribe', 'success', 'mood', 'pipes', 'friday', 'scaring', 'lovers', 'hue', 'umpire']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for t in tuples[:10]:\n",
        "  print(t[1])\n",
        "  sentences.append(t[1])\n",
        "  print(sentences)\n"
      ],
      "metadata": {
        "id": "nCIJ5EIGzzWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c71a53f-2e9e-43a0-a4c9-aacaa982af5d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['act', 'i']\n",
            "[['act', 'i']]\n",
            "['scene', 'i', 'london', 'the', 'palace']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace']]\n",
            "['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others']]\n",
            "['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care']]\n",
            "['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care'], ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant']]\n",
            "['and', 'breathe', 'short', 'winded', 'accents', 'of', 'new', 'broils']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care'], ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant'], ['and', 'breathe', 'short', 'winded', 'accents', 'of', 'new', 'broils']]\n",
            "['to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care'], ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant'], ['and', 'breathe', 'short', 'winded', 'accents', 'of', 'new', 'broils'], ['to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote']]\n",
            "['no', 'more', 'the', 'thirsty', 'entrance', 'of', 'this', 'soil']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care'], ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant'], ['and', 'breathe', 'short', 'winded', 'accents', 'of', 'new', 'broils'], ['to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote'], ['no', 'more', 'the', 'thirsty', 'entrance', 'of', 'this', 'soil']]\n",
            "['shall', 'daub', 'her', 'lips', 'with', 'her', 'own', 'children', 's', 'blood']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care'], ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant'], ['and', 'breathe', 'short', 'winded', 'accents', 'of', 'new', 'broils'], ['to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote'], ['no', 'more', 'the', 'thirsty', 'entrance', 'of', 'this', 'soil'], ['shall', 'daub', 'her', 'lips', 'with', 'her', 'own', 'children', 's', 'blood']]\n",
            "['nor', 'more', 'shall', 'trenching', 'war', 'channel', 'her', 'fields']\n",
            "[['act', 'i'], ['scene', 'i', 'london', 'the', 'palace'], ['enter', 'king', 'henry', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'of', 'westmoreland', 'sir', 'walter', 'blunt', 'and', 'others'], ['so', 'shaken', 'as', 'we', 'are', 'so', 'wan', 'with', 'care'], ['find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant'], ['and', 'breathe', 'short', 'winded', 'accents', 'of', 'new', 'broils'], ['to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote'], ['no', 'more', 'the', 'thirsty', 'entrance', 'of', 'this', 'soil'], ['shall', 'daub', 'her', 'lips', 'with', 'her', 'own', 'children', 's', 'blood'], ['nor', 'more', 'shall', 'trenching', 'war', 'channel', 'her', 'fields']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Term-Document Matrix [14 points]\n"
      ],
      "metadata": {
        "id": "_k_329Y6fmJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Creating Term-Document Matrix [10 points]"
      ],
      "metadata": {
        "id": "ciobf57D22EC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will write code to compile a term-document matrix for Shakespeare's plays, following the description in the textbook.\n",
        "\n",
        "> In a *term-document matrix*, each row represents a word in the vocabulary and each column represents a document from some collection. The figure below shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare. Each cell in this matrix represents the number of times a particular word (defined by the row) occurs in a particular document (defined by the column). Thus *clown* appeared 117 times in *Twelfth Night\n",
        "\n",
        "|             | As You Like It |  Twelfth Night  | Julias Caesar | Henry V |\n",
        "| :---------: |:--------------:| :-------------: | :----------:  | :-----: |\n",
        "| **battle**\t| 1 | 1 | 8 | 15 |\n",
        "| **soldier**\t| 2 | 2 | 12 | 36 |\n",
        "| **fool**\t\t| 37 | 58 | 1 | 5 |\n",
        "| **crown**\t\t| 5 | 117 | 0 | 0 |\n",
        "\n",
        "The dimensions of your term-document matrix will be the number of documents $D$ (in this case, the number of Shakespeare's plays that we give you in the corpus by the number of unique word types $\\|V\\|$ in that collection.   The columns represent the documents, and the rows represent the words, and each cell represents the frequency of that word in that document. "
      ],
      "metadata": {
        "id": "GJHZV0RWfryu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 1.1: Term Document Matrix** [10 points]\n",
        "\n",
        "    In your code you will write a function to `create_term_document_matrix`.  This will let you be the hit of your next dinner party by being able to answer trivia questions like *how many words did Shakespeare use?*, which may give us a hint to the answer to *How many words did Shakespeare know?*  The table will also tell you how many words Shakespeare used only once.  Did you know that there's a technical term for that?  In corpus linguistics they are called [*hapax legomena*](https://en.wikipedia.org/wiki/Hapax_legomenon), but I prefer the term *singleton*, because I don't like snooty Greek or Latin terms. \n"
      ],
      "metadata": {
        "id": "iAxVo_ADfuUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
        "\t'''Returns a numpy array containing the term document matrix for the input lines.\n",
        "\n",
        "\tInputs:\n",
        "\t\tline_tuples: A list of tuples, containing the name of the document and \n",
        "\t\ta tokenized line from that document.\n",
        "\t\tdocument_names: A list of the document names\n",
        "\t\tvocab: A list of the tokens in the vocabulary\n",
        "\n",
        "\t# NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
        "\n",
        "\tLet m = len(vocab) and n = len(document_names).\n",
        "\n",
        "\tReturns:\n",
        "\t\ttd_matrix: A mxn numpy array where the number of rows is the number of words\n",
        "\t\t\t\tand each column corresponds to a document. A_ij contains the\n",
        "\t\t\t\tfrequency with which word i occurs in document j.\n",
        "\t'''\n",
        "\n",
        "\tvocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
        "\tdocname_to_id = dict(zip(document_names, range(0, len(document_names))))\n",
        " \n",
        "\tm = len(vocab)\n",
        "\tn = len(document_names)\n",
        "\n",
        "\t# initialize a matrix with n/document columns, m/vocab rows\n",
        "\tmatrix  = np.zeros([m,n])\n",
        "\t\n",
        "\t# collapse words in the same document \n",
        "\tdoc_words = {}\n",
        "\tfor t in line_tuples:\n",
        "\t\t\tif t[0] not in doc_words:\n",
        "\t\t\t\tdoc_words[t[0]] = t[1]\n",
        "\t\t\tdoc_words[t[0]].extend(t[1])\n",
        "\n",
        "\t# count words in each document, store in a dictionary as {document_name: Counter object}\n",
        "\tcounter = {}\n",
        "\tfor doc in doc_words:\n",
        "\t\tcounter[doc] = Counter(doc_words[doc])\n",
        "\n",
        "\t# build the matrix\n",
        "\tfor docname, j in docname_to_id.items():\n",
        "\t\tfor vocab, i in vocab_to_id.items():\n",
        "\t\t\tfor doc in counter:\n",
        "\t\t\t\tif doc == docname:\n",
        "\t\t\t\t\tmatrix[i][j] = counter[doc][vocab] \n",
        "\n",
        "\t# YOUR CODE HERE\n",
        "\treturn matrix"
      ],
      "metadata": {
        "id": "6WYqK2qpfl2t"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuples, document_names, vocab = read_in_shakespeare()\n",
        "term_doc_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
        "term_doc_matrix.shape # should be (22602, 36)"
      ],
      "metadata": {
        "id": "NPtc2ZwIfno-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1a51e9-d2d1-48f6-b230-8f2e4ca61a08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22602, 36)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q11_td_matrix', answer = term_doc_matrix[:5000, :35]) # we only check partial data"
      ],
      "metadata": {
        "id": "wbMJDBJvkK5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0dc0fc3-5348-4841-f820-65f70891937e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 10/10 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Use Term-Document Matrix to Compare Documents [4 points]"
      ],
      "metadata": {
        "id": "E2egq5FR260M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The term-document matrix will also let us do cool things like figure out which plays are most similar to each other, by comparing the column vectors.  We could even look for outliers to see if some plays are so dissimilar from the rest of the canon that [maybe they weren't authored by Shakespeare after all](https://en.wikipedia.org/wiki/Shakespeare_authorship_question).  \n",
        "\n",
        "Let's begin by considering the column representing each play.  Each column is a $\\|V\\|$-dimensional vector.  Let's use some math to define the similarity of these vectors.   By far the most common similarity metric is the cosine of the angle between the vectors.  The cosine similarity metric is defined in Section 6.3 of the textbook.\n",
        "\n",
        "> The cosine, like most measures for vector similarity used in NLP, is based on the dot product operator from linear algebra, also called the inner product:\n",
        "\n",
        "> dot-product($\\vec{v}, \\vec{w}) = \\vec{v} \\cdot \\vec{w} = \\sum_{i=1}^{N}{v_iw_i} = v_1w_1 +v_2w_2 +...+v_Nw_N$\n",
        "\n",
        "> The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions (orthogonal vectors) will have a dot product of 0, representing their strong dissimilarity. \n",
        "\n",
        "> This raw dot-product, however, has a problem as a similarity metric: it favors long vectors. The vector length is defined as\n",
        "\n",
        "> $\\|\\vec{v}\\| = \\sqrt{\\sum_{i=1}^{N}{v_i^2}}$\n",
        "\n",
        "> The dot product is higher if a vector is longer, with higher values in each dimension. More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we would like a similarity metric that tells us how similar two words are regardless of their frequency.\n",
        "\n",
        "> The simplest way to modify the dot product to normalize for the vector length is to divide the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors, following from the definition of the dot product between two vectors $\\vec{v}$ and $\\vec{w}$ as:\n",
        "\n",
        "> $\\vec{v} \\cdot \\vec{w} = \\|\\vec{v}\\|\\|\\vec{w}\\| cos \\Theta$\n",
        "\n",
        "> $\\frac{\\vec{v} \\cdot \\vec{w}}{\\|\\vec{v}\\|\\|\\vec{w}\\|} =  cos \\Theta$\n",
        "\n",
        "> The cosine similarity metric between two vectors $\\vec{v}$ and $\\vec{w}$ thus can be computed\n",
        "\n",
        "> $cosine(\\vec{v}, \\vec{w}) = \\frac{\\vec{v} \\cdot \\vec{w}}{\\|\\vec{v}\\| \\|\\vec{w}\\|} = \\frac{\\sum_{i=1}^{N}{v_iw_i}}{\\sqrt{\\sum_{i=1}^{N}{v_i^2}} \\sqrt{\\sum_{i=1}^{N}{w_i^2}}} $\n",
        "\n",
        "The cosine value ranges from 1 for vectors pointing in the same direction, through 0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions. Since our term-document matrix contains raw frequency counts, it is non-negative, so the cosine for its vectors will range from 0 to 1.  1 means that the vectors are identical, 0 means that they are totally dissimilar.  \n"
      ],
      "metadata": {
        "id": "qBqU1bbe2wal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 1.2: Comparing plays**: Please implement `compute_cosine_similarity`, and for each play in the corpus, score how similar each other play is to it.  Which plays are the closet to each other in vector space (ignoring self similarity)?  Which plays are the most distant from each other? [4 points]\n"
      ],
      "metadata": {
        "id": "guL5_LW2kwqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(vector1, vector2):\n",
        "\t'''Computes the cosine similarity of the two input vectors.\n",
        "\n",
        "\tInputs:\n",
        "\t\tvector1: A nx1 numpy array\n",
        "\t\tvector2: A nx1 numpy array\n",
        "\n",
        "\tReturns:\n",
        "\t\tA scalar similarity value.\n",
        "\t'''\n",
        "\t \n",
        "\t# YOUR CODE HERE\n",
        "\tnorm_1 = np.sqrt(np.sum(vector1**2)) \n",
        "\tnorm_2 = np.sqrt(np.sum(vector2**2))\n",
        " \n",
        "\treturn np.dot(vector1,vector2)/(norm_1*norm_2)"
      ],
      "metadata": {
        "id": "K5jokxy2kwax"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(vector1, vector2):\n",
        "  '''Computes the cosine similarity of the two input vectors.\n",
        "\n",
        "  Inputs:\n",
        "    vector1: A nx1 numpy array\n",
        "    vector2: A nx1 numpy array\n",
        "\n",
        "  Returns:\n",
        "    A scalar similarity value.\n",
        "  '''\n",
        "  n1=int(vector1.T.dot(vector1))\n",
        "  n2=int(vector2.T.dot(vector2))\n",
        "  if(n1==0 or n2==0):  \n",
        "    sim = 0\n",
        "  else:\n",
        "    sim = float(vector1.T.dot(vector2))/(np.sqrt(n1)*np.sqrt(n2))\n",
        "  return sim"
      ],
      "metadata": {
        "id": "dDIaqnpAsYsb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q12_cos_sim', answer = compute_cosine_similarity)"
      ],
      "metadata": {
        "id": "gFBkVrjSkwTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740af34b-7c4a-417a-9436-7ae454293bb8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You earned 0/4 points.\n",
            "\n",
            "But, don't worry, you can re-submit and we will keep only your latest score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Measuring word similarity [10 points]\n",
        "Next, we're going to see how we can represent words as vectors in vector space.  This will give us a way of representing some aspects of the *meaning* of words, by measuring the similarity of their vectors. \n",
        "\n",
        "In our term-document matrix, the rows are word vectors.  Instead of a $\\|V\\|$-dimensional vector, these row vectors only have $D$ dimensions.  Do you think that's enough to represent the meaning of words? *(Spoiler Alert: no!)*\n"
      ],
      "metadata": {
        "id": "FGfGP4cxAFTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Term-Context Matrix [10 points]\n",
        "Instead of using a term-document matrix, a more common way of computing word similarity is by constructing a term-context matrix (also called a word-word matrix), where columns are labeled by words rather than documents.  The dimensionality of this kind of a matrix is $\\|V\\|$ by $\\|V\\|$.  Each cell represents how often the word in the row (the target word) co-occurs with the word in the column (the context) in a training corpus.  "
      ],
      "metadata": {
        "id": "opGvdaL7AH4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 2:** For this part of the assignment, you should write the `create_term_context_matrix` function.  This function specifies the size word window around the target word that you will use to gather its contexts.  For instance, if you set that variable to be 4, then you will use 4 words to the left of the target word, and 4 words to its right for the context.  In this case, the cell represents the number of times in Shakespeare's plays the column word occurs in +/-4 word window around the row word. [10 points]\n",
        "\n",
        "    You can now re-compute the most similar words for your test words using the row vectors in your term-context matrix instead of your term-document matrix.  What is the dimensionality of your word vectors now?  Do the most similar words make more sense than before?"
      ],
      "metadata": {
        "id": "T7I-gB4iAPds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
        "\t'''Returns a numpy array containing the term context matrix for the input lines.\n",
        "\n",
        "\tInputs:\n",
        "\t\tline_tuples: A list of tuples, containing the name of the document and \n",
        "\t\ta tokenized line from that document.\n",
        "\t\tvocab: A list of the tokens in the vocabulary\n",
        "\n",
        "\t# NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
        "\n",
        "\tLet n = len(vocab).\n",
        "\n",
        "\tReturns:\n",
        "\t\ttc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
        "\t\t\t\tword j was found within context_window_size to the left or right of\n",
        "\t\t\t\tword i in any sentence in the tuples.\n",
        "\t'''\n",
        "\tn = len(vocab)\n",
        "\tvocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
        "\n",
        "\t# YOUR CODE HERE\n",
        "\n",
        "\t# initialize a matrix, ‖𝑉‖ by ‖𝑉‖\n",
        "\tmatrix  = np.zeros([n,n])\n",
        "\t\n",
        "\t# collapse all words into one list, preserve words order\n",
        "\tsentences = []\n",
        "\tfor t in line_tuples:\n",
        "\t\t\tsentences.append(t[1])\n",
        "\n",
        "\t# build the matrix\n",
        "\tfor wrd_list in sentences:\n",
        "\t\tfor focus_wrd_indx, focus_wrd in enumerate(wrd_list):\n",
        "      # Get the indices of all the context words for the given focus word\n",
        "\t\t\tfor contxt_wrd_indx in range((max(0,focus_wrd_indx - context_window_size)),(min(len(wrd_list),focus_wrd_indx + context_window_size +1))):\n",
        "\t\t\t\tif wrd_list[contxt_wrd_indx] in vocab_to_id and focus_wrd in vocab_to_id and contxt_wrd_indx != focus_wrd_indx:\n",
        "\t\t\t\t\tword_index = vocab_to_id[focus_wrd]\n",
        "\t\t\t\t\tc_index = vocab_to_id[wrd_list[contxt_wrd_indx]]\n",
        "\t\t\t\t\tmatrix[word_index, c_index] += 1\n",
        "\n",
        "\treturn matrix"
      ],
      "metadata": {
        "id": "F4IZP5WPDZAH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuples, document_names, vocab = read_in_shakespeare()\n",
        "term_context_matrix = create_term_context_matrix(tuples, vocab, context_window_size = 2)\n",
        "term_context_matrix.shape # should be (22602, 22602)"
      ],
      "metadata": {
        "id": "d9UIxk7avqDr",
        "outputId": "d8e242d7-32e9-4fa3-cb5c-8a7d77c840c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22602, 22602)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "test_cases = [\n",
        "    [0, 0], # disliken, tribe\n",
        "    [1317, 5124], # the, sword\n",
        "    [15363, 10070], # love, you\n",
        "    [4601, 15242], # good, night\n",
        "    [8961, 6221], # hope, not\n",
        "]\n",
        "\n",
        "test_entries = [term_context_matrix[i, j] for i, j in test_cases]\n",
        "print(test_entries)\n",
        "grader.grade(test_case_id = 'test_q2_term_context_matrix', answer = test_entries)"
      ],
      "metadata": {
        "id": "nvpLzpwLAKIg",
        "outputId": "795bdc80-a28a-4d32-bfa3-8194c3996f22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 49.0, 168.0, 117.0, 6.0]\n",
            "Correct! You earned 10/10 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Weighting terms [28 points]\n",
        "Your term-context matrix contains the raw frequency of the co-occurrence of two words in each cell.  Raw frequency turns out not to be the best way of measuring the association between words.  There are several methods for weighting words so that we get better results.  You should implement two weighting schemes:\n",
        "\n",
        "* Positive pointwise mutual information (PPMI)\n",
        "* Term frequency inverse document frequency (tf-idf)\n",
        "\n",
        "These are defined in Section 6.2 of the textbook.\n",
        "\n",
        "*Warning, calculating PPMI for your whole $\\|V\\|$-by-$\\|V\\|$ matrix might be slow. Our intrepid TA's implementation for PPMI takes about 10 minutes to compute all values. She always writes perfectly optimized code on her first try. You may improve performance by using matrix operations a la MATLAB.*"
      ],
      "metadata": {
        "id": "tJlxuZ9bAKUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Weighting Schemes [20 points]"
      ],
      "metadata": {
        "id": "f_KRgsiK4J8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.1.1:** Implementing `create_PPMI_matrix` function [10 points]"
      ],
      "metadata": {
        "id": "8BPT2_nKGOBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.1.2:** Implementing `create_tf_idf_matrix` function [10 points]"
      ],
      "metadata": {
        "id": "M7PbTjbeW4On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_PPMI_matrix(term_context_matrix):\n",
        "\t'''Given a term context matrix, output a PPMI matrix.\n",
        "\t\n",
        "\tSee section 6.2 in the textbook.\n",
        "\t\n",
        "\tHint: Use numpy matrix and vector operations to speed up implementation.\n",
        "\t\t\t\tPlease also add a small constant 1e-6 to your term_context_matrix to avoid having 0s\n",
        "\t\n",
        "\tInput:\n",
        "\t\tterm_context_matrix: A nxn numpy array, where n is\n",
        "\t\t\t\tthe numer of tokens in the vocab.\n",
        "\t\n",
        "\tReturns: A nxn numpy matrix, where A_ij is equal to the\n",
        "\t\t point-wise mutual information between the ith word\n",
        "\t\t and the jth word in the term_context_matrix.\n",
        "\t'''       \n",
        "\t\n",
        "\t# YOUR CODE HERE\n",
        "\tterm_context_matrix += 1e-6\n",
        "\tjoint_prob = term_context_matrix/np.sum(term_context_matrix)\n",
        "\n",
        "\tword_C = np.sum(term_context_matrix, axis = 1)\n",
        "\tword_prob = word_C / np.sum(word_C)\n",
        "\n",
        "\tcontext_C = np.sum(term_context_matrix, axis = 0)\n",
        "\tcontext_prob =context_C/np.sum(context_C)\n",
        "\n",
        "\tjoint_exp = np.outer(word_prob, context_prob)\n",
        " \n",
        "\tPMI = np.log2(joint_prob/joint_exp)\n",
        "\tPMI[PMI < 0] = 0\n",
        "\treturn PMI\n",
        "\n",
        "def create_tf_idf_matrix(term_document_matrix):\n",
        "\t'''Given the term document matrix, output a tf-idf weighted version.\n",
        "\n",
        "\tSee section 6.2 in the textbook.\n",
        "\t\n",
        "\tHint: Use numpy matrix and vector operations to speed up implementation.\n",
        "\n",
        "\tInput:\n",
        "\t\tterm_document_matrix: Numpy array where each column represents a document \n",
        "\t\tand each row, the frequency of a word in that document.\n",
        "\n",
        "\tReturns:\n",
        "\t\tA numpy array with the same dimension as term_document_matrix, where\n",
        "\t\tA_ij is weighted by the inverse document frequency of document h.\n",
        "\t'''\n",
        "\n",
        "\t# YOUR CODE HERE\n",
        "\tN = term_document_matrix.shape[1]\n",
        "\tN = 1.0*N\n",
        "\tindicator = 1.0*(term_document_matrix > 0)\n",
        "\tdf = np.sum(indicator,axis=1)\n",
        "\tidf = np.log10(N/df)\n",
        "\tmatrix=np.multiply(term_document_matrix, np.transpose(np.array([idf,])))\n",
        "\n",
        "\treturn matrix "
      ],
      "metadata": {
        "id": "ar8rVLX_AKEH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Testing `create_tf_idf_matrix`"
      ],
      "metadata": {
        "id": "QpHHVnMO3xEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuples, document_names, vocab = read_in_shakespeare()\n",
        "term_doc_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
        "tfidf_matrix = create_tf_idf_matrix(term_doc_matrix)\n",
        "tfidf_matrix.shape # should be (22602, 36)"
      ],
      "metadata": {
        "id": "Jo-xoci4EHgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd425ba-091b-48df-b715-36e735469bf1"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22602, 36)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix[:5, :5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gExiUrM4fish",
        "outputId": "788c9515-d616-445b-8ecc-ad84c03867d9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 1.50514998, 0.        , 0.        , 1.20411998],\n",
              "       [0.32585358, 0.32585358, 0.        , 0.        , 0.32585358],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q31_tfidf_matrix', answer = tfidf_matrix[:1001, :31]) # we only check part of the answer so that we won't blow up the RAM"
      ],
      "metadata": {
        "id": "mblDfOuE3ipd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b42b6de-2023-4812-b9e3-616dada77745"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 10/10 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Testing `create_PPMI_matrix`"
      ],
      "metadata": {
        "id": "8s-Y8la9306-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuples, document_names, vocab = read_in_shakespeare()\n",
        "mini_vocab = ['dagger', 'run', 'the', 'bloody', 'sword'] # we only use part of the vocab so that you won't blow up the RAM\n",
        "term_context_matrix = create_term_context_matrix(tuples, mini_vocab, context_window_size = 2)\n",
        "PPMI_matrix = create_PPMI_matrix(term_context_matrix)\n",
        "PPMI_matrix.shape # should be (5, 5)"
      ],
      "metadata": {
        "id": "aXKz9nUD0Doz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8924a14d-7d0f-477b-f57e-a9cc0b710cbd"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q32_ppmi_matrix', answer = PPMI_matrix)"
      ],
      "metadata": {
        "id": "WzRz2vB1EHb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0c0a29-9220-435b-cf82-747677707a19"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 10/10 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Other Similarity Measurements [8 points]\n",
        "There are several ways of computing the similarity between two vectors.  In addition to writing a function to compute cosine similarity in Section 1, you should also write functions to `compute_jaccard_similarity` and `compute_dice_similarity`. You can check out the defintion of the [Jaccard measure here](https://en.wikipedia.org/wiki/Jaccard_index#Weighted_Jaccard_similarity_and_distance). And, dice similarity measure is given by (2 * J)/(J + 1) where J is Jaccard index. Please refer to this wikipedia link on [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) for more details.\n"
      ],
      "metadata": {
        "id": "cN-DLSlQ4M-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.2.1:** Implementing `jaccard` similarity [4 points]"
      ],
      "metadata": {
        "id": "jxI1yzNnGiHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_jaccard_similarity(vector1, vector2):\n",
        "\t'''Computes the cosine similarity of the two input vectors.\n",
        "\n",
        "\tInputs:\n",
        "\t\tvector1: A nx1 numpy array\n",
        "\t\tvector2: A nx1 numpy array\n",
        "\n",
        "\tReturns:\n",
        "\t\tA scalar similarity value.\n",
        "\t'''\n",
        "\t\n",
        "\t# YOUR CODE HERE\n",
        "\treturn np.sum(np.minimum(vector1,vector2))/(np.sum(np.maximum(vector1, vector2)))\n"
      ],
      "metadata": {
        "id": "3I_a7oCAEHaR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q33_jaccard_sim', answer = compute_jaccard_similarity)"
      ],
      "metadata": {
        "id": "JvDi9w7_5Z3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50480504-5f21-4564-cecb-b367567510c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 3.2.2:** Implementing `dice` similarity [4 points]"
      ],
      "metadata": {
        "id": "K-zYBDchXCY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_dice_similarity(vector1, vector2):\n",
        "\t'''Computes the cosine similarity of the two input vectors.\n",
        "\n",
        "\tInputs:\n",
        "\t\tvector1: A nx1 numpy array\n",
        "\t\tvector2: A nx1 numpy array\n",
        "\n",
        "\tReturns:\n",
        "\t\tA scalar similarity value.\n",
        "\t'''\n",
        "\n",
        "\t# YOUR CODE HERE\n",
        "\treturn np.sum(2*np.minimum(vector1,vector2))/(np.sum(vector1) + np.sum(vector2))"
      ],
      "metadata": {
        "id": "ivM-acyc4zHw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q34_dice_sim', answer = compute_dice_similarity)"
      ],
      "metadata": {
        "id": "Gu5BetXq4zBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2d959f-dbc8-4675-ba6f-e2b825773145"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Ranking [10 points]\n",
        "In this section, you will put everything together and **rank the playes and the words** based on their similarity to others.\n",
        "- **Problem 4.1:** Implement `rank_plays` [5 points]"
      ],
      "metadata": {
        "id": "gQEI9oUSE8M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_plays(target_play_index, term_document_matrix, similarity_fn):\n",
        "\t''' Ranks the similarity of all of the plays to the target play.\n",
        "\n",
        "\tInputs:\n",
        "\t\tdocument_names: List of document names, corresponding to  \n",
        "\t\t\tterm_document_matrix columns (i.e. name of document corresponding to \n",
        "\t\t\tterm_document_matrix[:,i] is given by document_names[i])\n",
        "\t\ttarget_play_index: The integer index of the play we want to compare all others against.\n",
        "\t\tterm_document_matrix: The term-document matrix as a mxn numpy array.\n",
        "\t\tsimilarity_fn: Function that should be used to compared vectors for two\n",
        "\t\t\tdocuments. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
        "\t\t\tcompute_cosine_similarity.\n",
        "\n",
        "\tReturns:\n",
        "\t\tA length-n list of strings corresponding to play names,\n",
        "\t\tordered by decreasing similarity to the play indexed by target_play_index\n",
        "\t'''\n",
        "\t# m: # of words; n: # of documents\n",
        "\tm, n = term_document_matrix.shape\n",
        "\t\n",
        "\t# YOUR CODE HERE\n",
        "\ttarget = term_document_matrix[:, target_play_index]\n",
        "\tsim_dic = dict((i, similarity_fn(target, term_document_matrix[:,i])) for i in range(n))\n",
        " \n",
        "\tsims_sort_dic = dict(sorted(sim_dic.items(), key=lambda item: item[1], reverse = True))\n",
        "\tsims_sort = np.array(list(sims_sort_dic.keys()))\n",
        "\treturn sims_sort"
      ],
      "metadata": {
        "id": "ubGEaq7sEHYW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE #\n",
        "term_doc_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
        "tfidf_matrix = create_tf_idf_matrix(term_doc_matrix)"
      ],
      "metadata": {
        "id": "VpOzaoiz7z8k"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Case - DO NOT CHANGE #\n",
        "target_play = 'Richard III'\n",
        "play_index = document_names.index(target_play)\n",
        "target_play_ranking = rank_plays(play_index, tfidf_matrix, compute_cosine_similarity)\n",
        "target_play_ranking.shape # should be a list of 36 numbers, representing the sorted similarity compared to the target play, Richard III"
      ],
      "metadata": {
        "id": "_HZqOif86mb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6ae92b-96fd-4ef0-a13c-78069892a0ee"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36,)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q41_play_ranking', answer = target_play_ranking)"
      ],
      "metadata": {
        "id": "_lIEo9AX7ws2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928d4c3b-09db-40ff-b65f-f401053bafb0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Problem 4.2:** Implement `rank_plays` [5 points]"
      ],
      "metadata": {
        "id": "x9LXnQxsXRXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_words(target_word_index, matrix, similarity_fn):\n",
        "\t''' Ranks the similarity of all of the words to the target word.\n",
        "\n",
        "\tInputs:\n",
        "\t\tvocab: List of terms, corresponding to target_word_index rows (i.e. word corresponding\n",
        "\t\t\tto target_word_index[i,:] is given by vocab[i])\n",
        "\t\ttarget_word_index: The index of the word we want to compare all others against.\n",
        "\t\tmatrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
        "\t\tsimilarity_fn: Function that should be used to compared vectors for two word\n",
        "\t\t\tebeddings. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
        "\t\t\tcompute_cosine_similarity.\n",
        "\n",
        "\tReturns:\n",
        "\t\tA length-n list of indices, ordered by decreasing similarity to the \n",
        "\t\ttarget word indexed by word_index\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\n",
        "\t# vocab number \n",
        "\tn, n = matrix.shape\n",
        "\n",
        "\ttarget = matrix[:, target_word_index]\n",
        "\tsims_dic = dict((i, similarity_fn(target, matrix[:, i])) for i in range(n))\n",
        "\tsims_sort_dic = dict(sorted(sims_dic.items(), key=lambda item: item[1], reverse = True))\n",
        "\tprint(list(sims_sort_dic.keys()))\n",
        "\tsims_sort = np.array(list(sims_sort_dic.keys()))\n",
        "\treturn sims_sort"
      ],
      "metadata": {
        "id": "7DRRQ5zw6h17"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Case - DO NOT CHANGE #\n",
        "tuples, document_names, vocab = read_in_shakespeare()\n",
        "N = 6000\n",
        "tmp_vocab = vocab[:N] # we only use part of the vocab so that you won't blow up the RAM\n",
        "term_context_matrix = create_term_context_matrix(tuples, tmp_vocab, context_window_size = 2)\n",
        "tmp_PPMI_matrix = create_PPMI_matrix(term_context_matrix)\n",
        "\n",
        "target_word = 'sword'\n",
        "i = tmp_vocab.index(target_word)\n",
        "\n",
        "target_word_ranking = rank_words(i, tmp_PPMI_matrix, compute_cosine_similarity)"
      ],
      "metadata": {
        "id": "AoLuzUCG741a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6883fcff-e66e-4b71-d325-fed5007d70ce"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5124, 4119, 2421, 1388, 5403, 5024, 4000, 796, 4750, 3931, 746, 4310, 3325, 3873, 4802, 4600, 5500, 236, 2944, 2978, 1263, 2929, 464, 1228, 3991, 308, 3347, 971, 242, 1653, 4625, 3260, 3005, 1216, 5367, 2767, 4013, 758, 2828, 1854, 2899, 4570, 3827, 2827, 2653, 4172, 1010, 203, 1785, 1462, 117, 3254, 4201, 4644, 4209, 1961, 3001, 2269, 5481, 475, 3120, 1880, 843, 5634, 633, 2313, 5169, 1657, 3765, 4426, 2854, 77, 5771, 5554, 2006, 416, 1366, 400, 5300, 313, 3252, 5449, 228, 1469, 2694, 3964, 5757, 1735, 5427, 295, 388, 5132, 3567, 332, 2993, 1053, 2677, 4280, 4122, 1788, 2373, 5759, 97, 3436, 5060, 2172, 811, 4272, 908, 929, 1969, 3456, 2151, 56, 684, 629, 1386, 2783, 276, 4361, 5299, 397, 4598, 1933, 3165, 1585, 849, 2387, 1592, 156, 1715, 2223, 894, 4649, 5301, 5236, 5704, 269, 924, 4214, 3798, 3529, 1095, 3009, 2433, 2293, 1867, 939, 1781, 3791, 910, 4237, 5981, 4017, 4582, 2143, 2578, 3863, 3227, 5917, 4350, 2361, 1349, 2627, 3442, 364, 4037, 2248, 2712, 4309, 4577, 1835, 405, 1480, 4574, 3997, 543, 4142, 3927, 2055, 4832, 2698, 2196, 4081, 3736, 4098, 3505, 5417, 318, 1844, 3241, 765, 5546, 3047, 1542, 3267, 1998, 3918, 396, 2499, 1407, 5677, 4303, 2583, 1682, 1428, 335, 501, 2281, 1993, 4087, 841, 2339, 4941, 647, 266, 3502, 4089, 4828, 1352, 5280, 1541, 4212, 48, 3422, 2231, 3844, 4234, 4887, 785, 3264, 493, 2882, 3472, 2985, 2257, 1067, 5698, 345, 3601, 2722, 365, 3685, 3865, 977, 575, 1997, 3813, 1317, 497, 4910, 531, 2934, 1908, 1900, 3437, 3762, 4818, 3521, 1186, 5514, 4564, 3906, 5276, 4129, 5078, 1827, 3166, 2600, 2094, 0, 11, 14, 16, 20, 21, 26, 29, 30, 43, 58, 63, 71, 82, 87, 90, 91, 92, 96, 99, 106, 109, 110, 114, 118, 120, 121, 127, 131, 140, 143, 158, 159, 170, 173, 178, 194, 195, 196, 205, 208, 217, 223, 229, 230, 244, 245, 255, 293, 300, 304, 310, 315, 323, 326, 334, 338, 343, 356, 362, 363, 366, 367, 368, 379, 383, 398, 399, 401, 406, 411, 424, 429, 438, 439, 440, 472, 476, 480, 481, 488, 512, 526, 534, 537, 546, 548, 550, 559, 563, 565, 576, 585, 600, 601, 607, 608, 632, 662, 668, 690, 697, 706, 709, 721, 730, 735, 738, 739, 740, 752, 756, 757, 760, 764, 770, 776, 777, 795, 801, 802, 810, 821, 823, 828, 836, 850, 853, 856, 857, 861, 867, 868, 880, 881, 883, 900, 906, 909, 911, 912, 917, 922, 923, 941, 949, 950, 967, 975, 980, 982, 984, 988, 991, 1003, 1006, 1008, 1023, 1024, 1026, 1031, 1035, 1045, 1046, 1050, 1057, 1064, 1091, 1102, 1104, 1105, 1109, 1110, 1112, 1114, 1115, 1120, 1126, 1129, 1149, 1162, 1164, 1177, 1182, 1193, 1194, 1203, 1204, 1205, 1208, 1210, 1212, 1226, 1232, 1238, 1240, 1246, 1268, 1290, 1292, 1295, 1311, 1312, 1321, 1323, 1330, 1336, 1340, 1359, 1360, 1368, 1385, 1389, 1392, 1397, 1406, 1412, 1419, 1429, 1438, 1442, 1446, 1461, 1465, 1494, 1502, 1503, 1508, 1509, 1510, 1516, 1517, 1525, 1526, 1533, 1537, 1540, 1543, 1550, 1554, 1564, 1575, 1604, 1606, 1621, 1622, 1626, 1644, 1663, 1668, 1670, 1675, 1685, 1690, 1697, 1698, 1706, 1718, 1722, 1734, 1742, 1743, 1759, 1768, 1774, 1783, 1793, 1794, 1800, 1804, 1805, 1806, 1808, 1815, 1825, 1828, 1831, 1840, 1845, 1847, 1848, 1859, 1860, 1865, 1896, 1902, 1909, 1916, 1930, 1957, 1959, 1979, 1985, 1989, 1990, 1994, 2000, 2001, 2005, 2007, 2009, 2017, 2027, 2035, 2039, 2043, 2045, 2066, 2076, 2101, 2105, 2106, 2109, 2111, 2117, 2120, 2122, 2125, 2128, 2144, 2147, 2154, 2173, 2178, 2179, 2189, 2192, 2199, 2201, 2209, 2214, 2237, 2238, 2241, 2244, 2246, 2247, 2251, 2253, 2256, 2259, 2263, 2267, 2271, 2275, 2278, 2280, 2294, 2298, 2303, 2309, 2316, 2320, 2331, 2334, 2347, 2348, 2351, 2354, 2365, 2369, 2380, 2383, 2395, 2403, 2406, 2430, 2432, 2435, 2436, 2438, 2441, 2442, 2457, 2459, 2461, 2464, 2479, 2482, 2491, 2523, 2528, 2530, 2532, 2542, 2544, 2561, 2573, 2576, 2585, 2587, 2589, 2592, 2598, 2599, 2606, 2624, 2625, 2629, 2633, 2636, 2637, 2638, 2651, 2652, 2663, 2671, 2675, 2676, 2681, 2682, 2685, 2706, 2729, 2737, 2744, 2751, 2754, 2755, 2765, 2770, 2785, 2787, 2788, 2794, 2796, 2798, 2801, 2803, 2808, 2809, 2810, 2812, 2817, 2823, 2831, 2832, 2836, 2843, 2855, 2857, 2859, 2874, 2888, 2890, 2909, 2911, 2913, 2914, 2915, 2926, 2928, 2930, 2933, 2937, 2939, 2941, 2954, 2955, 2963, 2972, 2977, 2988, 2995, 3018, 3034, 3035, 3044, 3053, 3054, 3068, 3076, 3078, 3081, 3086, 3095, 3104, 3121, 3126, 3128, 3129, 3138, 3141, 3144, 3150, 3160, 3170, 3173, 3175, 3176, 3192, 3195, 3203, 3214, 3221, 3224, 3225, 3228, 3235, 3238, 3240, 3287, 3290, 3296, 3320, 3321, 3328, 3331, 3333, 3335, 3336, 3344, 3348, 3357, 3359, 3360, 3370, 3378, 3381, 3401, 3406, 3408, 3411, 3414, 3417, 3419, 3427, 3430, 3443, 3445, 3451, 3454, 3466, 3477, 3487, 3496, 3512, 3531, 3554, 3555, 3559, 3570, 3577, 3578, 3592, 3594, 3602, 3603, 3611, 3620, 3621, 3623, 3624, 3632, 3633, 3643, 3644, 3648, 3654, 3655, 3656, 3660, 3663, 3664, 3670, 3672, 3678, 3690, 3703, 3712, 3715, 3722, 3725, 3735, 3750, 3753, 3770, 3771, 3790, 3794, 3800, 3816, 3822, 3825, 3831, 3842, 3845, 3849, 3854, 3864, 3889, 3891, 3900, 3908, 3912, 3921, 3922, 3924, 3925, 3929, 3930, 3940, 3947, 3970, 3971, 3976, 3983, 3989, 3993, 3999, 4008, 4009, 4012, 4026, 4027, 4028, 4030, 4036, 4044, 4053, 4055, 4056, 4065, 4066, 4082, 4094, 4097, 4108, 4116, 4121, 4124, 4133, 4159, 4186, 4202, 4207, 4215, 4217, 4221, 4229, 4246, 4250, 4253, 4254, 4256, 4258, 4262, 4277, 4292, 4311, 4316, 4317, 4324, 4329, 4335, 4338, 4342, 4344, 4353, 4358, 4368, 4370, 4373, 4376, 4378, 4379, 4383, 4415, 4420, 4427, 4433, 4436, 4439, 4449, 4452, 4456, 4458, 4461, 4467, 4468, 4470, 4471, 4475, 4478, 4481, 4491, 4499, 4500, 4509, 4520, 4524, 4525, 4526, 4540, 4552, 4558, 4589, 4590, 4602, 4608, 4614, 4623, 4629, 4630, 4646, 4650, 4651, 4655, 4656, 4661, 4663, 4675, 4705, 4717, 4719, 4720, 4721, 4725, 4727, 4732, 4735, 4737, 4742, 4743, 4755, 4757, 4762, 4767, 4776, 4803, 4805, 4816, 4820, 4821, 4822, 4826, 4831, 4835, 4839, 4848, 4849, 4854, 4859, 4865, 4866, 4870, 4873, 4875, 4876, 4877, 4878, 4880, 4881, 4888, 4902, 4907, 4919, 4922, 4928, 4932, 4933, 4960, 4961, 4962, 4969, 4973, 4978, 4988, 4994, 4995, 5003, 5005, 5011, 5015, 5020, 5022, 5039, 5047, 5082, 5085, 5086, 5089, 5090, 5103, 5104, 5114, 5118, 5120, 5121, 5127, 5148, 5154, 5158, 5160, 5172, 5181, 5198, 5213, 5229, 5232, 5234, 5237, 5238, 5241, 5249, 5261, 5263, 5272, 5277, 5281, 5286, 5288, 5291, 5294, 5295, 5308, 5314, 5320, 5323, 5334, 5352, 5354, 5375, 5376, 5392, 5399, 5401, 5407, 5415, 5419, 5423, 5428, 5446, 5450, 5465, 5472, 5479, 5495, 5499, 5504, 5511, 5521, 5524, 5534, 5543, 5544, 5548, 5550, 5552, 5562, 5584, 5586, 5588, 5590, 5592, 5595, 5597, 5619, 5620, 5627, 5639, 5646, 5657, 5664, 5668, 5687, 5709, 5716, 5722, 5724, 5727, 5747, 5758, 5763, 5768, 5778, 5783, 5795, 5812, 5813, 5820, 5824, 5826, 5831, 5832, 5854, 5859, 5866, 5873, 5877, 5893, 5911, 5929, 5930, 5935, 5944, 5947, 5949, 5953, 5956, 5957, 5963, 5964, 5968, 5970, 5976, 5995, 1913, 4002, 2282, 2208, 527, 2907, 2126, 5408, 4495, 3942, 2574, 4496, 3281, 1931, 4900, 2418, 523, 2183, 2648, 3208, 4595, 926, 139, 5998, 2338, 3067, 2018, 2960, 2772, 726, 4304, 1209, 1237, 2452, 5769, 4815, 1060, 5612, 5141, 4601, 81, 2713, 5353, 1213, 5170, 3572, 5993, 2864, 1040, 728, 4428, 4208, 2581, 1955, 2, 542, 5239, 574, 251, 4935, 452, 5245, 2376, 4095, 1221, 4070, 3356, 3646, 470, 4809, 1870, 5694, 5878, 319, 3699, 611, 4856, 3154, 5212, 4219, 5043, 2360, 351, 2431, 4669, 1047, 4401, 2417, 733, 2020, 1891, 544, 2265, 4177, 4594, 5845, 4145, 2037, 4216, 4645, 5822, 152, 4263, 5371, 5933, 4872, 2078, 5965, 78, 2792, 5138, 3467, 3080, 4348, 2412, 1752, 3626, 506, 3834, 4542, 3519, 4231, 1889, 3747, 4061, 1504, 4810, 2454, 3140, 3220, 3923, 4944, 1301, 5031, 4375, 925, 5614, 5083, 5870, 1156, 1259, 5684, 578, 1145, 4268, 2924, 1146, 5347, 4724, 4901, 2691, 568, 5386, 3872, 5839, 4925, 4402, 4693, 2215, 254, 500, 1423, 712, 4658, 4247, 2936, 889, 246, 1367, 5183, 3777, 5721, 4101, 1055, 5918, 5393, 2260, 3832, 172, 2736, 1500, 2562, 5493, 412, 3497, 4489, 5019, 5515, 846, 5435, 104, 1966, 2605, 2414, 5977, 3880, 2643, 4191, 3262, 2982, 5189, 2885, 4407, 2236, 5460, 3405, 1125, 676, 5516, 5642, 1403, 2424, 1765, 2507, 2952, 2390, 4584, 1293, 2356, 5143, 5596, 5107, 4723, 2026, 2456, 5081, 1387, 4112, 681, 2315, 4963, 4075, 5644, 320, 5568, 5766, 4971, 4130, 2170, 1512, 263, 2489, 3387, 5418, 1893, 1384, 3695, 3045, 4536, 3637, 1719, 1693, 2493, 2526, 2868, 162, 4503, 5617, 420, 4591, 4192, 5155, 530, 2515, 4731, 2081, 5028, 3196, 4140, 5267, 2880, 8, 3985, 2446, 3732, 4748, 3111, 80, 2805, 2067, 819, 3941, 1689, 4490, 4956, 111, 4286, 4125, 2065, 5804, 3962, 3188, 3481, 3460, 1910, 902, 642, 4501, 5056, 190, 5594, 734, 3364, 1134, 1562, 1988, 2742, 1253, 5396, 1647, 2077, 2426, 187, 2053, 1498, 1332, 4700, 7, 2866, 5879, 4823, 990, 5754, 39, 1544, 4243, 436, 790, 4657, 3795, 232, 4146, 5040, 167, 3322, 1546, 1070, 4455, 4535, 2229, 503, 2554, 1027, 2475, 1485, 937, 5119, 3026, 38, 4265, 105, 951, 220, 3835, 3973, 1972, 2746, 2484, 3040, 1298, 1729, 5053, 3178, 683, 938, 2593, 4633, 2234, 5402, 301, 4539, 4514, 3520, 3384, 5793, 3379, 243, 2670, 5071, 1144, 1591, 1496, 5610, 2073, 1726, 2062, 5811, 2702, 4886, 4084, 4463, 426, 1864, 557, 4395, 1353, 3015, 621, 5744, 2533, 2743, 3418, 854, 5848, 5345, 5144, 4137, 855, 2363, 2614, 1315, 4553, 2632, 2453, 3870, 1310, 2235, 3146, 835, 5444, 354, 696, 4462, 3533, 336, 3177, 3213, 5672, 5319, 67, 1350, 3596, 1087, 699, 5545, 1379, 896, 2622, 4441, 968, 2998, 5360, 1289, 341, 4102, 4576, 2865, 3679, 3380, 3119, 252, 2131, 2118, 5814, 5988, 1394, 3590, 3075, 3051, 4659, 123, 3995, 4114, 1558, 2894, 4320, 2848, 4862, 2048, 4408, 3388, 3151, 933, 3361, 1720, 2130, 3, 4493, 1266, 1160, 227, 2410, 3933, 4240, 2327, 2502, 5776, 2752, 593, 1466, 2830, 431, 5821, 5601, 3183, 732, 5153, 1555, 594, 1630, 199, 5539, 953, 5806, 1277, 1658, 4667, 5233, 650, 693, 4575, 1907, 4738, 2791, 2156, 970, 2113, 875, 1143, 5626, 946, 3314, 3511, 5469, 3368, 3513, 5250, 344, 5128, 3892, 5010, 1769, 2211, 4567, 1166, 680, 3763, 1829, 4555, 5471, 3340, 3904, 714, 945, 328, 5257, 3768, 3894, 842, 5782, 4393, 2272, 4296, 5304, 3551, 3642, 661, 5613, 2967, 5540, 15, 5013, 4444, 389, 188, 2242, 4688, 5218, 4355, 4139, 679, 2818, 1687, 5901, 192, 2762, 2419, 4599, 5055, 4410, 3557, 886, 4537, 2428, 5675, 1, 3222, 2724, 2940, 3158, 491, 5710, 4616, 5342, 2112, 4267, 5959, 5585, 3063, 5325, 4390, 4314, 4916, 5712, 371, 1054, 1736, 4173, 3274, 393, 125, 57, 3282, 4466, 1404, 182, 2546, 4049, 3311, 3952, 3773, 3446, 2626, 239, 2325, 2336, 787, 1044, 837, 1219, 5094, 1612, 5363, 2607, 5420, 3137, 5097, 2476, 1995, 2509, 3390, 620, 5840, 5036, 4200, 2255, 4312, 2132, 206, 1062, 483, 5424, 569, 1963, 2123, 5248, 4299, 1101, 4289, 1180, 5475, 4435, 5456, 4578, 1639, 191, 5805, 1453, 274, 2693, 4714, 3548, 2892, 4628, 3326, 2727, 2790, 2887, 4571, 4798, 5214, 4726, 445, 1646, 1275, 3649, 2976, 3108, 1184, 5992, 5726, 5753, 2802, 1447, 1767, 2103, 1133, 1271, 4365, 4772, 5027, 4804, 5222, 5662, 3489, 2480, 5570, 3537, 272, 4741, 2057, 1244, 2878, 3571, 3350, 4825, 2375, 3465, 2312, 5147, 5112, 3342, 3615, 1482, 921, 5909, 2276, 2416, 3302, 5880, 652, 5412, 4406, 4740, 2485, 180, 2527, 3211, 3308, 4005, 1600, 2811, 3535, 4871, 1417, 277, 4770, 3412, 5697, 598, 5488, 3792, 3975, 1038, 5203, 655, 3156, 717, 827, 4696, 2443, 1173, 4541, 2311, 1111, 3589, 751, 5954, 1678, 154, 3946, 3839, 2594, 2667, 4409, 5313, 2158, 3179, 4155, 169, 2270, 4165, 998, 216, 2498, 3811, 5569, 871, 3022, 3761, 5305, 1871, 3606, 2919, 3400, 3285, 3052, 2330, 5023, 4004, 4249, 4666, 4523, 89, 943, 1898, 5327, 5767, 1155, 516, 722, 2136, 5165, 1030, 2324, 4391, 1331, 1716, 4294, 4678, 5941, 2322, 435, 864, 4236, 3884, 1881, 1764, 5942, 3640, 249, 2797, 2023, 5666, 433, 1607, 130, 1286, 2814, 5, 5931, 1684, 5385, 994, 1534, 5018, 1217, 4840, 3560, 660, 4351, 1937, 4977, 1925, 4223, 5560, 2739, 2997, 5192, 4032, 3748, 5001, 1476, 4422, 5852, 5108, 4632, 5856, 2608, 4483, 4264, 2902, 5841, 5786, 5715, 956, 3142, 4068, 2098, 1073, 4938, 4387, 1886, 5621, 112, 5269, 4686, 1787, 641, 3087, 4912, 5032, 108, 2851, 3079, 4991, 3272, 443, 1858, 3048, 1304, 2284, 3628, 2216, 3848, 5881, 1838, 1899, 852, 1399, 2946, 5708, 2690, 2824, 4836, 5922, 2912, 1196, 5566, 3134, 4789, 1917, 742, 1887, 3002, 3702, 5914, 5049, 5725, 5958, 3248, 4163, 5655, 4544, 992, 2404, 2971, 5685, 5219, 773, 4923, 13, 3661, 2680, 5661, 3612, 4561, 3136, 874, 5734, 839, 5400, 2301, 3812, 3276, 5271, 4242, 3516, 1308, 3217, 5130, 174, 3720, 5289, 2429, 4974, 895, 2728, 2618, 3011, 1454, 289, 1927, 1798, 3826, 1842, 3752, 5835, 1464, 1041, 4874, 2321, 74, 643, 4689, 915, 93, 1677, 667, 1401, 3023, 5983, 4692, 3099, 2780, 4060, 3724, 638, 2841, 3565, 3049, 1348, 2344, 2060, 2146, 2198, 753, 1141, 4323, 5366, 1956, 5829, 1797, 4855, 513, 5226, 5557, 4430, 2249, 5034, 3503, 1444, 1837, 4099, 4519, 2377, 5689, 2012, 2115, 4228, 4830, 5228, 3247, 2469, 5985, 5803, 2165, 1468, 4007, 3391, 700, 489, 3515, 4154, 4384, 1597, 1757, 2709, 5202, 3569, 3895, 4841, 744, 1603, 1425, 3898, 183, 5473, 5106, 5361, 2439, 5924, 5174, 5215, 4983, 4058, 1588, 3587, 2807, 5962, 2462, 5682, 1556, 103, 5587, 4328, 2063, 3266, 4890, 2740, 5137, 4782, 4083, 3107, 5080, 5755, 28, 1581, 4278, 2494, 3804, 2959, 555, 2366, 2730, 5382, 1395, 41, 5150, 4506, 4990, 5206, 4512, 1574, 3041, 2425, 5006, 2861, 5912, 1620, 2708, 1071, 4185, 2597, 455, 4747, 5100, 1571, 474, 2833, 1287, 2413, 2150, 3092, 1316, 4913, 1328, 4513, 4050, 3463, 1696, 3719, 3988, 4842, 1869, 2460, 5925, 1713, 53, 3680, 1416, 5126, 1664, 5636, 4765, 2569, 3582, 5220, 654, 626, 175, 340, 5850, 2371, 5883, 599, 2975, 3977, 1683, 1876, 1589, 3444, 656, 3323, 1711, 4260, 3056, 3802, 2699, 3242, 1179, 4174, 1195, 2965, 387, 5991, 4363, 3972, 3392, 5834, 3486, 2813, 1863, 3913, 2615, 664, 5875, 2467, 3089, 3341, 3803, 4453, 4269, 651, 5333, 613, 3184, 4160, 2623, 4404, 5748, 5509, 279, 1590, 4382, 4438, 4676, 2753, 2217, 2034, 1458, 3103, 4697, 5737, 4071, 1375, 2872, 4662, 1405, 4497, 5284, 2718, 2455, 5139, 1817, 612, 2392, 731, 2931, 2560, 221, 352, 605, 1318, 1374, 1536, 1547, 1671, 1776, 2240, 2481, 2660, 3070, 3473, 3651, 3869, 3890, 4096, 4302, 5017, 5283, 5600, 5606, 5863, 1561, 2061, 1255, 1249, 2393, 3297, 778, 3666, 4241, 1490, 4105, 5679, 1487, 5780, 447, 2114, 3148, 5225, 3510, 386, 609, 560, 4613, 4307, 3317, 4431, 2968, 5801, 1472, 519, 1629, 2335, 3758, 4924, 692, 3949, 570, 510, 2635, 2654, 278, 3251, 2799, 1431, 5338, 4448, 882, 1977, 3760, 579, 4413, 3216, 4957, 4883, 5533, 5346, 5749, 149, 5997, 552, 4795, 798, 5652, 3981, 5073, 18, 1666, 3385, 1108, 2273, 4203, 1637, 2577, 3662, 463, 4531, 4684, 1924, 2490, 3676, 1161, 1584, 1437, 5818, 5335, 1174, 4076, 4326, 4169, 2920, 1032, 3097, 273, 891, 1474, 3339, 3249, 961, 5459, 4180, 2212, 3050, 1016, 4808, 1313, 1598, 2440, 3027, 4067, 4465, 4791, 5035, 5889, 1982, 2355, 865, 138, 4811, 772, 4024, 805, 5972, 2107, 5624, 2088, 3315, 847, 4473, 5692, 4476, 2254, 2342, 2776, 5735, 2678, 1457, 2603, 1075, 665, 928, 1305, 2058, 2300, 4665, 5117, 3963, 747, 1552, 5461, 1242, 2575, 1234, 2853, 2225, 3996, 3944, 4529, 3528, 2847, 4029, 851, 1175, 3372, 2219, 5076, 2162, 5565, 3830, 2564, 5381, 3566, 4377, 3162, 677, 2568, 1980, 432, 2434, 657, 4847, 518, 1197, 3469, 2508, 3638, 1414, 4617, 5217, 2873, 280, 4181, 469, 5223, 2697, 3153, 4546, 944, 1190, 5536, 3210, 4781, 325, 4572, 4609, 4771, 5038, 5470, 5986, 2555, 4583, 4713, 3353, 558, 653, 3201, 3728, 2368, 2089, 3731, 2054, 441, 1511, 959, 2074, 61, 64, 153, 423, 509, 561, 628, 688, 830, 1206, 1381, 1411, 1577, 1679, 1737, 1760, 1796, 1862, 1905, 1906, 1946, 1967, 2096, 2185, 2202, 2274, 2358, 2402, 2405, 2497, 2557, 2580, 2726, 2756, 3189, 3425, 3440, 3475, 3538, 3780, 3883, 3936, 4144, 4244, 4534, 4606, 4722, 4744, 4774, 4884, 4911, 4970, 5167, 5224, 5285, 5292, 5529, 5645, 5815, 294, 3741, 1573, 2906, 70, 1061, 3950, 5227, 5088, 1282, 3478, 4560, 5632, 3862, 3755, 2483, 2075, 250, 3682, 2947, 66, 986, 2224, 4690, 3772, 283, 2317, 2261, 4946, 5084, 5531, 2662, 327, 1539, 2891, 1168, 976, 1601, 3579, 5413, 3164, 3232, 5876, 2584, 899, 669, 3152, 2203, 1475, 1981, 2881, 1369, 562, 348, 2420, 5062, 129, 4141, 2996, 2364, 5525, 3244, 2705, 4640, 4143, 5913, 164, 955, 997, 3684, 4386, 1709, 5608, 1879, 5002, 5492, 144, 235, 3055, 3307, 1521, 1314, 511, 3926, 3671, 5860, 468, 2538, 5974, 5868, 2905, 4411, 1278, 4295, 3767, 5633, 2445, 1553, 1656, 1456, 2108, 3706, 4346, 5133, 4638, 2349, 663, 2133, 553, 2152, 4085, 3365, 62, 2357, 800, 3453, 1202, 5575, 314, 5021, 2768, 3327, 2145, 1491, 2218, 2741, 4559, 4863, 4702, 3042, 532, 1513, 5455, 5486, 5131, 2092, 1288, 3806, 1572, 4284, 3468, 3711, 3205, 3345, 4508, 5310, 1118, 1563, 5251, 996, 3495, 5422, 1717, 1791, 2195, 2775, 3098, 4607, 4325, 3821, 5542, 4587, 2033, 4252, 1894, 4850, 5179, 446, 572, 1739, 1033, 4502, 3902, 2511, 1947, 1136, 5950, 2398, 3157, 3858, 1579, 2397, 4710, 76, 1227, 2837, 312, 5442, 35, 940, 1756, 4653, 1779, 3004, 3069, 3709, 3683, 5849, 3507, 3954, 2520, 1068, 4136, 65, 5828, 3349, 2733, 5321, 4711, 2649, 2991, 3301, 1022, 3951, 4846, 5478, 4281, 2444, 3117, 5751, 1866, 2353, 4897, 2086, 1642, 2686, 3190, 5282, 4858, 4745, 2979, 31, 4733, 5908, 5975, 5329, 5946, 5134, 2388, 3292, 4699, 1080, 4110, 498, 4851, 2901, 4197, 4975, 1609, 2821, 258, 1850, 3017, 5176, 1251, 5770, 2962, 2711, 4369, 4834, 3960, 4381, 1714, 771, 1063, 4182, 3817, 5625, 1868, 5921, 5362, 5208, 1890, 4405, 5686, 5906, 1269, 3064, 2961, 4681, 1836, 5105, 5275, 3094, 2994, 357, 2072, 3787, 3123, 1772, 3595, 4768, 4128, 4794, 4149, 5647, 40, 4001, 5994, 4354, 3194, 4318, 5230, 1058, 2974, 2408, 1154, 317, 4605, 720, 4364, 4954, 1329, 2910, 1627, 1493, 2307, 5846, 3061, 2579, 122, 1471, 3395, 4204, 2155, 1309, 2701, 5303, 2264, 3283, 4797, 157, 1952, 5703, 4914, 4388, 5799, 711, 3718, 1051, 1667, 5210, 1843, 37, 1549, 528, 374, 5896, 707, 5784, 5373, 5838, 5670, 425, 6, 2815, 3820, 2700, 2642, 1810, 3223, 2871, 2715, 1364, 4270, 4769, 3866, 3038, 5497, 5390, 2308, 5654, 3938, 1430, 3928, 3072, 147, 1507, 2826, 3037, 3088, 3423, 3953, 5920, 769, 482, 55, 5681, 2738, 1619, 1100, 3066, 1322, 658, 4245, 884, 671, 5072, 4034, 5902, 5487, 5583, 5522, 1351, 3805, 4515, 211, 4305, 484, 807, 1615, 3147, 3338, 3586, 4093, 930, 3982, 4080, 3159, 5939, 659, 4259, 4611, 2536, 581, 5129, 1699, 3688, 4374, 762, 5855, 3377, 1017, 456, 2465, 5553, 3647, 3610, 3639, 4103, 4127, 4565, 9, 4235, 1042, 630, 5341, 3730, 202, 4135, 1056, 1012, 4111, 4225, 5404, 1638, 5434, 5641, 4824, 5387, 5899, 5809, 4330, 782, 3115, 171, 2134, 640, 5339, 5151, 2287, 3691, 4777, 179, 4367, 45, 5330, 2386, 3701, 4088, 4779, 1215, 4168, 1021, 2688, 107, 4951, 627, 3948, 83, 1245, 1672, 2582, 5665, 5318, 533, 372, 5510, 5200, 1335, 142, 361, 410, 1084, 1515, 1568, 1721, 1790, 3410, 3874, 4226, 4521, 5185, 5411, 5464, 5819, 4440, 2870, 4274, 1090, 1247, 3106, 983, 1727, 5255, 2180, 494, 1911, 1872, 2239, 3652, 1153, 797, 838, 4642, 2759, 1223, 5000, 226, 623, 783, 888, 1124, 1211, 1250, 1274, 1535, 1662, 1704, 1914, 2021, 2233, 2721, 2763, 3039, 3204, 3236, 3837, 3917, 4123, 4343, 4687, 5099, 5109, 5266, 5364, 5467, 5800, 780, 4290, 5739, 2382, 3774, 989, 5635, 1875, 963, 885, 1901, 3253, 724, 3801, 17, 3539, 1636, 2193, 804, 4903, 5746, 892, 2175, 1738, 1777, 4048, 4313, 2782, 287, 2070, 2389, 2895, 3230, 5009, 5398, 5823, 2401, 1354, 4399, 5733, 1986, 2819, 5050, 3471, 745, 4337, 5490, 2262, 3109, 5389, 2668, 4360, 1750, 5541, 3897, 2844, 3850, 3278, 5774, 586, 1820, 958, 437, 502, 1159, 4079, 5745, 1083, 729, 1941, 4300, 4670, 5312, 3759, 5656, 3261, 2778, 460, 4945, 3986, 4685, 4889, 5890, 3501, 4021, 564, 3939, 3143, 816, 2535, 5162, 4167, 3604, 2602, 2319, 580, 1884, 3219, 3525, 2036, 3605, 3255, 2222, 3740, 1613, 2619, 573, 36, 1824, 786, 3482, 591, 5195, 297, 1960, 3182, 5115, 3122, 5102, 5394, 2304, 4504, 418, 1723, 1420, 2849, 5201, 5414, 3288, 3438, 3062, 4929, 1356, 4547, 5426, 964, 5659, 4092, 4949, 763, 5598, 3485, 2250, 347, 1230, 1436, 3057, 2838, 2153, 1754, 1641, 1002, 3334, 5395, 1634, 5973, 2129, 1611, 4118, 3373, 5808, 3907, 5377, 238, 3029, 1280, 4785, 3653, 5302, 822, 2346, 4634, 803, 2896, 4885, 1938, 3167, 3568, 1324, 4543, 166, 1333, 160, 5827, 1582, 5207, 1334, 541, 1745, 4443, 2139, 3810, 1882, 1610, 4527, 2964, 102, 3404, 3448, 5631, 522, 2596, 2703, 1614, 5702, 587, 3352, 695, 5706, 3310, 5623, 2252, 5498, 873, 4170, 775, 378, 260, 3212, 3518, 1272, 3967, 1254, 3498, 1382, 3434, 3172, 2893, 4498, 3916, 2835, 3243, 2628, 4682, 1669, 1633, 3543, 132, 870, 1139, 113, 454, 743, 901, 1013, 1092, 2226, 2749, 2840, 3246, 3583, 4018, 4042, 4708, 4792, 4906, 5052, 5063, 5484, 5559, 5714, 5788, 5978, 927, 1398, 1680, 1915, 2059, 2359, 2473, 3316, 3488, 4814, 5173, 60, 863, 1393, 3846, 2617, 47, 214, 267, 453, 920, 1557, 1846, 2171, 2184, 3133, 3439, 3452, 3522, 3840, 5738, 1489, 2563, 3376, 4210, 209, 2127, 2687, 1239, 4981, 3754, 5246, 1834, 2448, 876, 2806, 1346, 10, 788, 4861, 2367, 4352, 540, 5259, 1762, 4845, 2571, 3573, 972, 4206, 5025, 5344, 2258, 1654, 3416, 4126, 4984, 4992, 5356, 5867, 1488, 5077, 1434, 4706, 3083, 1830, 1559, 5643, 2630, 4482, 2647, 2177, 1005, 4959, 848, 644, 1267, 2559, 231, 5061, 1391, 5156, 4793, 5270, 5765, 2822, 4349, 2332, 5688, 1992, 3318, 987, 5482, 3965, 4759, 2167, 5785, 5618, 5520, 1773, 3065, 4671, 5551, 290, 5489, 444, 5979, 46, 51, 52, 79, 163, 253, 291, 302, 316, 322, 380, 384, 385, 392, 421, 428, 448, 457, 467, 473, 490, 508, 549, 597, 614, 624, 631, 698, 710, 727, 748, 761, 792, 812, 814, 833, 834, 859, 879, 913, 936, 954, 978, 1069, 1099, 1130, 1198, 1201, 1218, 1235, 1248, 1257, 1261, 1337, 1344, 1413, 1426, 1483, 1492, 1501, 1532, 1594, 1623, 1651, 1778, 1802, 1892, 1951, 1974, 1975, 1976, 1983, 1991, 2014, 2032, 2042, 2052, 2083, 2160, 2310, 2329, 2374, 2409, 2427, 2447, 2458, 2514, 2524, 2550, 2572, 2609, 2611, 2644, 2659, 2665, 2710, 2779, 2829, 2852, 2862, 2904, 2917, 2949, 2984, 3021, 3028, 3046, 3114, 3149, 3206, 3229, 3237, 3257, 3279, 3305, 3369, 3398, 3470, 3508, 3517, 3523, 3540, 3580, 3591, 3608, 3613, 3629, 3636, 3686, 3687, 3727, 3742, 3776, 3781, 3860, 3861, 3885, 3934, 3966, 3984, 4014, 4025, 4051, 4078, 4156, 4195, 4211, 4248, 4275, 4279, 4321, 4334, 4394, 4432, 4442, 4516, 4538, 4618, 4619, 4844, 4868, 4899, 4904, 4930, 4943, 4953, 4955, 4966, 4985, 4987, 5007, 5042, 5074, 5161, 5196, 5240, 5252, 5311, 5351, 5365, 5384, 5410, 5433, 5451, 5462, 5480, 5505, 5591, 5671, 5676, 5691, 5717, 5741, 5742, 5797, 5810, 5817, 5898, 5948, 5980, 5989, 2010, 5098, 767, 4622, 305, 5221, 5397, 635, 4138, 2422, 4677, 5370, 2124, 2604, 808, 5057, 1530, 4879, 4673, 2757, 670, 1151, 948, 271, 115, 184, 931, 1523, 2285, 3313, 5622, 5743, 135, 307, 321, 346, 462, 701, 815, 817, 866, 932, 934, 1093, 1107, 1199, 1297, 1435, 1495, 1580, 1616, 1665, 1674, 1730, 1818, 1841, 1950, 2161, 2187, 2296, 2717, 2845, 2983, 2990, 3013, 3016, 3329, 3402, 3491, 3499, 3550, 3635, 3708, 3721, 3764, 3843, 3886, 4020, 4022, 4189, 4224, 4604, 4636, 4683, 4718, 4864, 5067, 5190, 5242, 5432, 5523, 5572, 5683, 5864, 5871, 5938, 5987, 3616, 101, 210, 2923, 4454, 736, 794, 4222, 4756, 5982, 177, 4445, 4778, 5349, 1486, 1948, 2981, 3362, 4134, 4937, 5615, 5707, 2102, 973, 1347, 1373, 4758, 5293, 784, 1749, 4621, 5491, 5149, 2750, 2732, 381, 2148, 3403, 893, 1813, 3231, 3992, 4980, 3868, 2008, 12, 3337, 539, 2210, 5026, 3355, 1128, 3713, 5260, 3698, 5630, 2793, 4843, 1074, 1903, 5033, 5421, 1855, 5095, 3575, 3074, 4233, 1519, 3700, 3786, 3450, 4615, 2747, 3169, 4964, 1978, 2142, 181, 1795, 869, 4477, 4736, 2674, 3399, 4998, 1299, 3675, 3526, 2664, 4176, 2645, 1545, 1094, 4915, 3490, 4035, 2471, 161, 3073, 1484, 2038, 168, 247, 2232, 2986, 3969, 4680, 4472, 914, 1439, 2748, 2842, 1077, 3945, 5907, 2186, 4288, 3738, 779, 2362, 4003, 1635, 4175, 1377, 5058, 5730, 2716, 4852, 3546, 465, 119, 5836, 1188, 5340, 2295, 5518, 524, 4996, 3779, 3937, 1345, 4164, 4562, 1294, 3426, 5253, 904, 3096, 1987, 809, 4184, 507, 4227, 1617, 4624, 5014, 3116, 373, 2856, 3744, 4637, 5204, 5406, 5790, 4549, 5966, 2548, 5332, 2545, 5337, 1256, 3306, 4494, 42, 5700, 4011, 1052, 2657, 3980, 2950, 1302, 5629, 3459, 5513, 1833, 1043, 5087, 495, 306, 1934, 5452, 916, 3033, 5166, 4031, 390, 1452, 133, 4072, 3909, 682, 616, 1809, 1327, 2011, 4648, 2646, 285, 1415, 466, 3857, 3563, 5732, 1117, 1127, 1912, 2029, 1700, 1784, 1020, 1823, 3544, 2858, 2168, 1200, 4183, 5199, 2570, 2121, 3110, 2850, 4488, 5142, 458, 4892, 4198, 5287, 2068, 5517, 2305, 4450, 3259, 5191, 2610, 2921, 5574, 5429, 126, 5163, 2784, 1954, 1273, 1171, 3911, 5851, 422, 1953, 567, 2846, 3785, 995, 5171, 3609, 5431, 5336, 5378, 1459, 350, 5660, 4276, 1761, 4107, 2140, 2085, 2534, 4287, 826, 4701, 5064, 4396, 5468, 1640, 2925, 4976, 5564, 1631, 4464, 5205, 3354, 324, 1499, 820, 5116, 5182, 4729, 1814, 1378, 2758, 947, 5447, 2631, 737, 704, 4282, 2190, 2306, 3545, 4763, 4672, 5441, 3145, 5844, 5760, 1004, 5934, 4385, 5231, 5713, 2205, 1578, 339, 4783, 2916, 1072, 4194, 212, 5507, 3008, 2683, 4507, 1421, 257, 409, 3429, 5696, 3514, 750, 514, 4120, 813, 4709, 1285, 4694, 2521, 4703, 1812, 5436, 5830, 3090, 54, 2213, 4033, 2221, 3309, 5211, 2181, 2966, 2884, 3968, 4728, 5887, 5937, 5773, 4451, 2056, 5315, 1701, 155, 2291, 577, 137, 2658, 5640, 590, 3809, 4838, 4641, 1477, 4860, 1001, 3324, 2326, 2999, 1522, 791, 4333, 2197, 1932, 303, 4647, 3295, 687, 5882, 5736, 2400, 2470, 4882, 3903, 4487, 5101, 5440, 5825, 2350, 1628, 918, 1748, 517, 201, 4199, 88, 4920, 2948, 3031, 1467, 1599, 3667, 3641, 1260, 1819, 2191, 1821, 3271, 3574, 4239, 1326, 3500, 2019, 3871, 3746, 1852, 957, 5331, 5628, 4, 19, 22, 23, 24, 25, 27, 32, 33, 34, 44, 49, 50, 59, 68, 69, 72, 73, 75, 84, 85, 86, 94, 95, 98, 100, 116, 124, 128, 134, 136, 141, 145, 146, 148, 150, 151, 165, 176, 185, 186, 189, 193, 197, 198, 200, 204, 207, 213, 215, 218, 219, 222, 224, 225, 233, 234, 237, 240, 241, 248, 256, 259, 261, 262, 264, 265, 268, 270, 275, 281, 282, 284, 286, 288, 292, 296, 298, 299, 309, 311, 329, 330, 331, 333, 337, 342, 349, 353, 355, 358, 359, 360, 369, 370, 375, 376, 377, 382, 391, 394, 395, 402, 403, 404, 407, 408, 413, 414, 415, 417, 419, 427, 430, 434, 442, 449, 450, 451, 459, 461, 471, 477, 478, 479, 485, 486, 487, 492, 496, 499, 504, 505, 515, 520, 521, 525, 529, 535, 536, 538, 545, 547, 551, 554, 556, 566, 571, 582, 583, 584, 588, 589, 592, 595, 596, 602, 603, 604, 606, 610, 615, 617, 618, 619, 622, 625, 634, 636, 637, 639, 645, 646, 648, 649, 666, 672, 673, 674, 675, 678, 685, 686, 689, 691, 694, 702, 703, 705, 708, 713, 715, 716, 718, 719, 723, 725, 741, 749, 754, 755, 759, 766, 768, 774, 781, 789, 793, 799, 806, 818, 824, 825, 829, 831, 832, 840, 844, 845, 858, 860, 862, 872, 877, 878, 887, 890, 897, 898, 903, 905, 907, 919, 935, 942, 952, 960, 962, 965, 966, 969, 974, 979, 981, 985, 993, 999, 1000, 1007, 1009, 1011, 1014, 1015, 1018, 1019, 1025, 1028, 1029, 1034, 1036, 1037, 1039, 1048, 1049, 1059, 1065, 1066, 1076, 1078, 1079, 1081, 1082, 1085, 1086, 1088, 1089, 1096, 1097, 1098, 1103, 1106, 1113, 1116, 1119, 1121, 1122, 1123, 1131, 1132, 1135, 1137, 1138, 1140, 1142, 1147, 1148, 1150, 1152, 1157, 1158, 1163, 1165, 1167, 1169, 1170, 1172, 1176, 1178, 1181, 1183, 1185, 1187, 1189, 1191, 1192, 1207, 1214, 1220, 1222, 1224, 1225, 1229, 1231, 1233, 1236, 1241, 1243, 1252, 1258, 1262, 1264, 1265, 1270, 1276, 1279, 1281, 1283, 1284, 1291, 1296, 1300, 1303, 1306, 1307, 1319, 1320, 1325, 1338, 1339, 1341, 1342, 1343, 1355, 1357, 1358, 1361, 1362, 1363, 1365, 1370, 1371, 1372, 1376, 1380, 1383, 1390, 1396, 1400, 1402, 1408, 1409, 1410, 1418, 1422, 1424, 1427, 1432, 1433, 1440, 1441, 1443, 1445, 1448, 1449, 1450, 1451, 1455, 1460, 1463, 1470, 1473, 1478, 1479, 1481, 1497, 1505, 1506, 1514, 1518, 1520, 1524, 1527, 1528, 1529, 1531, 1538, 1548, 1551, 1560, 1565, 1566, 1567, 1569, 1570, 1576, 1583, 1586, 1587, 1593, 1595, 1596, 1602, 1605, 1608, 1618, 1624, 1625, 1632, 1643, 1645, 1648, 1649, 1650, 1652, 1655, 1659, 1660, 1661, 1673, 1676, 1681, 1686, 1688, 1691, 1692, 1694, 1695, 1702, 1703, 1705, 1707, 1708, 1710, 1712, 1724, 1725, 1728, 1731, 1732, 1733, 1740, 1741, 1744, 1746, 1747, 1751, 1753, 1755, 1758, 1763, 1766, 1770, 1771, 1775, 1780, 1782, 1786, 1789, 1792, 1799, 1801, 1803, 1807, 1811, 1816, 1822, 1826, 1832, 1839, 1849, 1851, 1853, 1856, 1857, 1861, 1873, 1874, 1877, 1878, 1883, 1885, 1888, 1895, 1897, 1904, 1918, 1919, 1920, 1921, 1922, 1923, 1926, 1928, 1929, 1935, 1936, 1939, 1940, 1942, 1943, 1944, 1945, 1949, 1958, 1962, 1964, 1965, 1968, 1970, 1971, 1973, 1984, 1996, 1999, 2002, 2003, 2004, 2013, 2015, 2016, 2022, 2024, 2025, 2028, 2030, 2031, 2040, 2041, 2044, 2046, 2047, 2049, 2050, 2051, 2064, 2069, 2071, 2079, 2080, 2082, 2084, 2087, 2090, 2091, 2093, 2095, 2097, 2099, 2100, 2104, 2110, 2116, 2119, 2135, 2137, 2138, 2141, 2149, 2157, 2159, 2163, 2164, 2166, 2169, 2174, 2176, 2182, 2188, 2194, 2200, 2204, 2206, 2207, 2220, 2227, 2228, 2230, 2243, 2245, 2266, 2268, 2277, 2279, 2283, 2286, 2288, 2289, 2290, 2292, 2297, 2299, 2302, 2314, 2318, 2323, 2328, 2333, 2337, 2340, 2341, 2343, 2345, 2352, 2370, 2372, 2378, 2379, 2381, 2384, 2385, 2391, 2394, 2396, 2399, 2407, 2411, 2415, 2423, 2437, 2449, 2450, 2451, 2463, 2466, 2468, 2472, 2474, 2477, 2478, 2486, 2487, 2488, 2492, 2495, 2496, 2500, 2501, 2503, 2504, 2505, 2506, 2510, 2512, 2513, 2516, 2517, 2518, 2519, 2522, 2525, 2529, 2531, 2537, 2539, 2540, 2541, 2543, 2547, 2549, 2551, 2552, 2553, 2556, 2558, 2565, 2566, 2567, 2586, 2588, 2590, 2591, 2595, 2601, 2612, 2613, 2616, 2620, 2621, 2634, 2639, 2640, 2641, 2650, 2655, 2656, 2661, 2666, 2669, 2672, 2673, 2679, 2684, 2689, 2692, 2695, 2696, 2704, 2707, 2714, 2719, 2720, 2723, 2725, 2731, 2734, 2735, 2745, 2760, 2761, 2764, 2766, 2769, 2771, 2773, 2774, 2777, 2781, 2786, 2789, 2795, 2800, 2804, 2816, 2820, 2825, 2834, 2839, 2860, 2863, 2867, 2869, 2875, 2876, 2877, 2879, 2883, 2886, 2889, 2897, 2898, 2900, 2903, 2908, 2918, 2922, 2927, 2932, 2935, 2938, 2942, 2943, 2945, 2951, 2953, 2956, 2957, 2958, 2969, 2970, 2973, 2980, 2987, 2989, 2992, 3000, 3003, 3006, 3007, 3010, 3012, 3014, 3019, 3020, 3024, 3025, 3030, 3032, 3036, 3043, 3058, 3059, 3060, 3071, 3077, 3082, 3084, 3085, 3091, 3093, 3100, 3101, 3102, 3105, 3112, 3113, 3118, 3124, 3125, 3127, 3130, 3131, 3132, 3135, 3139, 3155, 3161, 3163, 3168, 3171, 3174, 3180, 3181, 3185, 3186, 3187, 3191, 3193, 3197, 3198, 3199, 3200, 3202, 3207, 3209, 3215, 3218, 3226, 3233, 3234, 3239, 3245, 3250, 3256, 3258, 3263, 3265, 3268, 3269, 3270, 3273, 3275, 3277, 3280, 3284, 3286, 3289, 3291, 3293, 3294, 3298, 3299, 3300, 3303, 3304, 3312, 3319, 3330, 3332, 3343, 3346, 3351, 3358, 3363, 3366, 3367, 3371, 3374, 3375, 3382, 3383, 3386, 3389, 3393, 3394, 3396, 3397, 3407, 3409, 3413, 3415, 3420, 3421, 3424, 3428, 3431, 3432, 3433, 3435, 3441, 3447, 3449, 3455, 3457, 3458, 3461, 3462, 3464, 3474, 3476, 3479, 3480, 3483, 3484, 3492, 3493, 3494, 3504, 3506, 3509, 3524, 3527, 3530, 3532, 3534, 3536, 3541, 3542, 3547, 3549, 3552, 3553, 3556, 3558, 3561, 3562, 3564, 3576, 3581, 3584, 3585, 3588, 3593, 3597, 3598, 3599, 3600, 3607, 3614, 3617, 3618, 3619, 3622, 3625, 3627, 3630, 3631, 3634, 3645, 3650, 3657, 3658, 3659, 3665, 3668, 3669, 3673, 3674, 3677, 3681, 3689, 3692, 3693, 3694, 3696, 3697, 3704, 3705, 3707, 3710, 3714, 3716, 3717, 3723, 3726, 3729, 3733, 3734, 3737, 3739, 3743, 3745, 3749, 3751, 3756, 3757, 3766, 3769, 3775, 3778, 3782, 3783, 3784, 3788, 3789, 3793, 3796, 3797, 3799, 3807, 3808, 3814, 3815, 3818, 3819, 3823, 3824, 3828, 3829, 3833, 3836, 3838, 3841, 3847, 3851, 3852, 3853, 3855, 3856, 3859, 3867, 3875, 3876, 3877, 3878, 3879, 3881, 3882, 3887, 3888, 3893, 3896, 3899, 3901, 3905, 3910, 3914, 3915, 3919, 3920, 3932, 3935, 3943, 3955, 3956, 3957, 3958, 3959, 3961, 3974, 3978, 3979, 3987, 3990, 3994, 3998, 4006, 4010, 4015, 4016, 4019, 4023, 4038, 4039, 4040, 4041, 4043, 4045, 4046, 4047, 4052, 4054, 4057, 4059, 4062, 4063, 4064, 4069, 4073, 4074, 4077, 4086, 4090, 4091, 4100, 4104, 4106, 4109, 4113, 4115, 4117, 4131, 4132, 4147, 4148, 4150, 4151, 4152, 4153, 4157, 4158, 4161, 4162, 4166, 4171, 4178, 4179, 4187, 4188, 4190, 4193, 4196, 4205, 4213, 4218, 4220, 4230, 4232, 4238, 4251, 4255, 4257, 4261, 4266, 4271, 4273, 4283, 4285, 4291, 4293, 4297, 4298, 4301, 4306, 4308, 4315, 4319, 4322, 4327, 4331, 4332, 4336, 4339, 4340, 4341, 4345, 4347, 4356, 4357, 4359, 4362, 4366, 4371, 4372, 4380, 4389, 4392, 4397, 4398, 4400, 4403, 4412, 4414, 4416, 4417, 4418, 4419, 4421, 4423, 4424, 4425, 4429, 4434, 4437, 4446, 4447, 4457, 4459, 4460, 4469, 4474, 4479, 4480, 4484, 4485, 4486, 4492, 4505, 4510, 4511, 4517, 4518, 4522, 4528, 4530, 4532, 4533, 4545, 4548, 4550, 4551, 4554, 4556, 4557, 4563, 4566, 4568, 4569, 4573, 4579, 4580, 4581, 4585, 4586, 4588, 4592, 4593, 4596, 4597, 4603, 4610, 4612, 4620, 4626, 4627, 4631, 4635, 4639, 4643, 4652, 4654, 4660, 4664, 4668, 4674, 4679, 4691, 4695, 4698, 4704, 4707, 4712, 4715, 4716, 4730, 4734, 4739, 4746, 4749, 4751, 4752, 4753, 4754, 4760, 4761, 4764, 4766, 4773, 4775, 4780, 4784, 4786, 4787, 4788, 4790, 4796, 4799, 4800, 4801, 4806, 4807, 4812, 4813, 4817, 4819, 4827, 4829, 4833, 4837, 4853, 4857, 4867, 4869, 4891, 4893, 4894, 4895, 4896, 4898, 4905, 4908, 4909, 4917, 4918, 4921, 4926, 4927, 4931, 4934, 4936, 4939, 4940, 4942, 4947, 4948, 4950, 4952, 4958, 4965, 4967, 4968, 4972, 4979, 4982, 4986, 4989, 4993, 4997, 4999, 5004, 5008, 5012, 5016, 5029, 5030, 5037, 5041, 5044, 5045, 5046, 5048, 5051, 5054, 5059, 5065, 5066, 5068, 5069, 5070, 5075, 5079, 5091, 5092, 5093, 5096, 5110, 5111, 5113, 5122, 5123, 5125, 5135, 5136, 5140, 5145, 5146, 5152, 5157, 5159, 5164, 5168, 5175, 5177, 5178, 5180, 5184, 5186, 5187, 5188, 5193, 5194, 5197, 5209, 5216, 5235, 5243, 5244, 5247, 5254, 5256, 5258, 5262, 5264, 5265, 5268, 5273, 5274, 5278, 5279, 5290, 5296, 5297, 5298, 5306, 5307, 5309, 5316, 5317, 5322, 5324, 5326, 5328, 5343, 5348, 5350, 5355, 5357, 5358, 5359, 5368, 5369, 5372, 5374, 5379, 5380, 5383, 5388, 5391, 5405, 5409, 5416, 5425, 5430, 5437, 5438, 5439, 5443, 5445, 5448, 5453, 5454, 5457, 5458, 5463, 5466, 5474, 5476, 5477, 5483, 5485, 5494, 5496, 5501, 5502, 5503, 5506, 5508, 5512, 5519, 5526, 5527, 5528, 5530, 5532, 5535, 5537, 5538, 5547, 5549, 5555, 5556, 5558, 5561, 5563, 5567, 5571, 5573, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5589, 5593, 5599, 5602, 5603, 5604, 5605, 5607, 5609, 5611, 5616, 5637, 5638, 5648, 5649, 5650, 5651, 5653, 5658, 5663, 5667, 5669, 5673, 5674, 5678, 5680, 5690, 5693, 5695, 5699, 5701, 5705, 5711, 5718, 5719, 5720, 5723, 5728, 5729, 5731, 5740, 5750, 5752, 5756, 5761, 5762, 5764, 5772, 5775, 5777, 5779, 5781, 5787, 5789, 5791, 5792, 5794, 5796, 5798, 5802, 5807, 5816, 5833, 5837, 5842, 5843, 5847, 5853, 5857, 5858, 5861, 5862, 5865, 5869, 5872, 5874, 5884, 5885, 5886, 5888, 5891, 5892, 5894, 5895, 5897, 5900, 5903, 5904, 5905, 5910, 5915, 5916, 5919, 5923, 5926, 5927, 5928, 5932, 5936, 5940, 5943, 5945, 5951, 5952, 5955, 5960, 5961, 5967, 5969, 5971, 5984, 5990, 5996, 5999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_word = 'sword'\n",
        "i = tmp_vocab.index(target_word)\n",
        "\n",
        "target_word_ranking = rank_words(i, tmp_PPMI_matrix, compute_cosine_similarity)\n",
        "# target_word_ranking[:5]\n",
        "np.delete(target_word_ranking, 3, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCuK7gW4Glhv",
        "outputId": "c4b164a2-cc5d-4866-830d-6da2b95bdd02"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5124, 4119, 2421, 1388, 5403, 5024, 4000, 796, 4750, 3931, 746, 4310, 3325, 3873, 4802, 4600, 5500, 236, 2944, 2978, 1263, 2929, 464, 1228, 3991, 308, 3347, 971, 242, 1653, 4625, 3260, 3005, 1216, 5367, 2767, 4013, 758, 2828, 1854, 2899, 4570, 3827, 2827, 2653, 4172, 1010, 203, 1785, 1462, 117, 3254, 4201, 4644, 4209, 1961, 3001, 2269, 5481, 475, 3120, 1880, 843, 5634, 633, 2313, 5169, 1657, 3765, 4426, 2854, 77, 5771, 5554, 2006, 416, 1366, 400, 5300, 313, 3252, 5449, 228, 1469, 2694, 3964, 5757, 1735, 5427, 295, 388, 5132, 3567, 332, 2993, 1053, 2677, 4280, 4122, 1788, 2373, 5759, 97, 3436, 5060, 2172, 811, 4272, 908, 929, 1969, 3456, 2151, 56, 684, 629, 1386, 2783, 276, 4361, 5299, 397, 4598, 1933, 3165, 1585, 849, 2387, 1592, 156, 1715, 2223, 894, 4649, 5301, 5236, 5704, 269, 924, 4214, 3798, 3529, 1095, 3009, 2433, 2293, 1867, 939, 1781, 3791, 910, 4237, 5981, 4017, 4582, 2143, 2578, 3863, 3227, 5917, 4350, 2361, 1349, 2627, 3442, 364, 4037, 2248, 2712, 4309, 4577, 1835, 405, 1480, 4574, 3997, 543, 4142, 3927, 2055, 4832, 2698, 2196, 4081, 3736, 4098, 3505, 5417, 318, 1844, 3241, 765, 5546, 3047, 1542, 3267, 1998, 3918, 396, 2499, 1407, 5677, 4303, 2583, 1682, 1428, 335, 501, 2281, 1993, 4087, 841, 2339, 4941, 647, 266, 3502, 4089, 4828, 1352, 5280, 1541, 4212, 48, 3422, 2231, 3844, 4234, 4887, 785, 3264, 493, 2882, 3472, 2985, 2257, 1067, 5698, 345, 3601, 2722, 365, 3685, 3865, 977, 575, 1997, 3813, 1317, 497, 4910, 531, 2934, 1908, 1900, 3437, 3762, 4818, 3521, 1186, 5514, 4564, 3906, 5276, 4129, 5078, 1827, 3166, 2600, 2094, 0, 11, 14, 16, 20, 21, 26, 29, 30, 43, 58, 63, 71, 82, 87, 90, 91, 92, 96, 99, 106, 109, 110, 114, 118, 120, 121, 127, 131, 140, 143, 158, 159, 170, 173, 178, 194, 195, 196, 205, 208, 217, 223, 229, 230, 244, 245, 255, 293, 300, 304, 310, 315, 323, 326, 334, 338, 343, 356, 362, 363, 366, 367, 368, 379, 383, 398, 399, 401, 406, 411, 424, 429, 438, 439, 440, 472, 476, 480, 481, 488, 512, 526, 534, 537, 546, 548, 550, 559, 563, 565, 576, 585, 600, 601, 607, 608, 632, 662, 668, 690, 697, 706, 709, 721, 730, 735, 738, 739, 740, 752, 756, 757, 760, 764, 770, 776, 777, 795, 801, 802, 810, 821, 823, 828, 836, 850, 853, 856, 857, 861, 867, 868, 880, 881, 883, 900, 906, 909, 911, 912, 917, 922, 923, 941, 949, 950, 967, 975, 980, 982, 984, 988, 991, 1003, 1006, 1008, 1023, 1024, 1026, 1031, 1035, 1045, 1046, 1050, 1057, 1064, 1091, 1102, 1104, 1105, 1109, 1110, 1112, 1114, 1115, 1120, 1126, 1129, 1149, 1162, 1164, 1177, 1182, 1193, 1194, 1203, 1204, 1205, 1208, 1210, 1212, 1226, 1232, 1238, 1240, 1246, 1268, 1290, 1292, 1295, 1311, 1312, 1321, 1323, 1330, 1336, 1340, 1359, 1360, 1368, 1385, 1389, 1392, 1397, 1406, 1412, 1419, 1429, 1438, 1442, 1446, 1461, 1465, 1494, 1502, 1503, 1508, 1509, 1510, 1516, 1517, 1525, 1526, 1533, 1537, 1540, 1543, 1550, 1554, 1564, 1575, 1604, 1606, 1621, 1622, 1626, 1644, 1663, 1668, 1670, 1675, 1685, 1690, 1697, 1698, 1706, 1718, 1722, 1734, 1742, 1743, 1759, 1768, 1774, 1783, 1793, 1794, 1800, 1804, 1805, 1806, 1808, 1815, 1825, 1828, 1831, 1840, 1845, 1847, 1848, 1859, 1860, 1865, 1896, 1902, 1909, 1916, 1930, 1957, 1959, 1979, 1985, 1989, 1990, 1994, 2000, 2001, 2005, 2007, 2009, 2017, 2027, 2035, 2039, 2043, 2045, 2066, 2076, 2101, 2105, 2106, 2109, 2111, 2117, 2120, 2122, 2125, 2128, 2144, 2147, 2154, 2173, 2178, 2179, 2189, 2192, 2199, 2201, 2209, 2214, 2237, 2238, 2241, 2244, 2246, 2247, 2251, 2253, 2256, 2259, 2263, 2267, 2271, 2275, 2278, 2280, 2294, 2298, 2303, 2309, 2316, 2320, 2331, 2334, 2347, 2348, 2351, 2354, 2365, 2369, 2380, 2383, 2395, 2403, 2406, 2430, 2432, 2435, 2436, 2438, 2441, 2442, 2457, 2459, 2461, 2464, 2479, 2482, 2491, 2523, 2528, 2530, 2532, 2542, 2544, 2561, 2573, 2576, 2585, 2587, 2589, 2592, 2598, 2599, 2606, 2624, 2625, 2629, 2633, 2636, 2637, 2638, 2651, 2652, 2663, 2671, 2675, 2676, 2681, 2682, 2685, 2706, 2729, 2737, 2744, 2751, 2754, 2755, 2765, 2770, 2785, 2787, 2788, 2794, 2796, 2798, 2801, 2803, 2808, 2809, 2810, 2812, 2817, 2823, 2831, 2832, 2836, 2843, 2855, 2857, 2859, 2874, 2888, 2890, 2909, 2911, 2913, 2914, 2915, 2926, 2928, 2930, 2933, 2937, 2939, 2941, 2954, 2955, 2963, 2972, 2977, 2988, 2995, 3018, 3034, 3035, 3044, 3053, 3054, 3068, 3076, 3078, 3081, 3086, 3095, 3104, 3121, 3126, 3128, 3129, 3138, 3141, 3144, 3150, 3160, 3170, 3173, 3175, 3176, 3192, 3195, 3203, 3214, 3221, 3224, 3225, 3228, 3235, 3238, 3240, 3287, 3290, 3296, 3320, 3321, 3328, 3331, 3333, 3335, 3336, 3344, 3348, 3357, 3359, 3360, 3370, 3378, 3381, 3401, 3406, 3408, 3411, 3414, 3417, 3419, 3427, 3430, 3443, 3445, 3451, 3454, 3466, 3477, 3487, 3496, 3512, 3531, 3554, 3555, 3559, 3570, 3577, 3578, 3592, 3594, 3602, 3603, 3611, 3620, 3621, 3623, 3624, 3632, 3633, 3643, 3644, 3648, 3654, 3655, 3656, 3660, 3663, 3664, 3670, 3672, 3678, 3690, 3703, 3712, 3715, 3722, 3725, 3735, 3750, 3753, 3770, 3771, 3790, 3794, 3800, 3816, 3822, 3825, 3831, 3842, 3845, 3849, 3854, 3864, 3889, 3891, 3900, 3908, 3912, 3921, 3922, 3924, 3925, 3929, 3930, 3940, 3947, 3970, 3971, 3976, 3983, 3989, 3993, 3999, 4008, 4009, 4012, 4026, 4027, 4028, 4030, 4036, 4044, 4053, 4055, 4056, 4065, 4066, 4082, 4094, 4097, 4108, 4116, 4121, 4124, 4133, 4159, 4186, 4202, 4207, 4215, 4217, 4221, 4229, 4246, 4250, 4253, 4254, 4256, 4258, 4262, 4277, 4292, 4311, 4316, 4317, 4324, 4329, 4335, 4338, 4342, 4344, 4353, 4358, 4368, 4370, 4373, 4376, 4378, 4379, 4383, 4415, 4420, 4427, 4433, 4436, 4439, 4449, 4452, 4456, 4458, 4461, 4467, 4468, 4470, 4471, 4475, 4478, 4481, 4491, 4499, 4500, 4509, 4520, 4524, 4525, 4526, 4540, 4552, 4558, 4589, 4590, 4602, 4608, 4614, 4623, 4629, 4630, 4646, 4650, 4651, 4655, 4656, 4661, 4663, 4675, 4705, 4717, 4719, 4720, 4721, 4725, 4727, 4732, 4735, 4737, 4742, 4743, 4755, 4757, 4762, 4767, 4776, 4803, 4805, 4816, 4820, 4821, 4822, 4826, 4831, 4835, 4839, 4848, 4849, 4854, 4859, 4865, 4866, 4870, 4873, 4875, 4876, 4877, 4878, 4880, 4881, 4888, 4902, 4907, 4919, 4922, 4928, 4932, 4933, 4960, 4961, 4962, 4969, 4973, 4978, 4988, 4994, 4995, 5003, 5005, 5011, 5015, 5020, 5022, 5039, 5047, 5082, 5085, 5086, 5089, 5090, 5103, 5104, 5114, 5118, 5120, 5121, 5127, 5148, 5154, 5158, 5160, 5172, 5181, 5198, 5213, 5229, 5232, 5234, 5237, 5238, 5241, 5249, 5261, 5263, 5272, 5277, 5281, 5286, 5288, 5291, 5294, 5295, 5308, 5314, 5320, 5323, 5334, 5352, 5354, 5375, 5376, 5392, 5399, 5401, 5407, 5415, 5419, 5423, 5428, 5446, 5450, 5465, 5472, 5479, 5495, 5499, 5504, 5511, 5521, 5524, 5534, 5543, 5544, 5548, 5550, 5552, 5562, 5584, 5586, 5588, 5590, 5592, 5595, 5597, 5619, 5620, 5627, 5639, 5646, 5657, 5664, 5668, 5687, 5709, 5716, 5722, 5724, 5727, 5747, 5758, 5763, 5768, 5778, 5783, 5795, 5812, 5813, 5820, 5824, 5826, 5831, 5832, 5854, 5859, 5866, 5873, 5877, 5893, 5911, 5929, 5930, 5935, 5944, 5947, 5949, 5953, 5956, 5957, 5963, 5964, 5968, 5970, 5976, 5995, 1913, 4002, 2282, 2208, 527, 2907, 2126, 5408, 4495, 3942, 2574, 4496, 3281, 1931, 4900, 2418, 523, 2183, 2648, 3208, 4595, 926, 139, 5998, 2338, 3067, 2018, 2960, 2772, 726, 4304, 1209, 1237, 2452, 5769, 4815, 1060, 5612, 5141, 4601, 81, 2713, 5353, 1213, 5170, 3572, 5993, 2864, 1040, 728, 4428, 4208, 2581, 1955, 2, 542, 5239, 574, 251, 4935, 452, 5245, 2376, 4095, 1221, 4070, 3356, 3646, 470, 4809, 1870, 5694, 5878, 319, 3699, 611, 4856, 3154, 5212, 4219, 5043, 2360, 351, 2431, 4669, 1047, 4401, 2417, 733, 2020, 1891, 544, 2265, 4177, 4594, 5845, 4145, 2037, 4216, 4645, 5822, 152, 4263, 5371, 5933, 4872, 2078, 5965, 78, 2792, 5138, 3467, 3080, 4348, 2412, 1752, 3626, 506, 3834, 4542, 3519, 4231, 1889, 3747, 4061, 1504, 4810, 2454, 3140, 3220, 3923, 4944, 1301, 5031, 4375, 925, 5614, 5083, 5870, 1156, 1259, 5684, 578, 1145, 4268, 2924, 1146, 5347, 4724, 4901, 2691, 568, 5386, 3872, 5839, 4925, 4402, 4693, 2215, 254, 500, 1423, 712, 4658, 4247, 2936, 889, 246, 1367, 5183, 3777, 5721, 4101, 1055, 5918, 5393, 2260, 3832, 172, 2736, 1500, 2562, 5493, 412, 3497, 4489, 5019, 5515, 846, 5435, 104, 1966, 2605, 2414, 5977, 3880, 2643, 4191, 3262, 2982, 5189, 2885, 4407, 2236, 5460, 3405, 1125, 676, 5516, 5642, 1403, 2424, 1765, 2507, 2952, 2390, 4584, 1293, 2356, 5143, 5596, 5107, 4723, 2026, 2456, 5081, 1387, 4112, 681, 2315, 4963, 4075, 5644, 320, 5568, 5766, 4971, 4130, 2170, 1512, 263, 2489, 3387, 5418, 1893, 1384, 3695, 3045, 4536, 3637, 1719, 1693, 2493, 2526, 2868, 162, 4503, 5617, 420, 4591, 4192, 5155, 530, 2515, 4731, 2081, 5028, 3196, 4140, 5267, 2880, 8, 3985, 2446, 3732, 4748, 3111, 80, 2805, 2067, 819, 3941, 1689, 4490, 4956, 111, 4286, 4125, 2065, 5804, 3962, 3188, 3481, 3460, 1910, 902, 642, 4501, 5056, 190, 5594, 734, 3364, 1134, 1562, 1988, 2742, 1253, 5396, 1647, 2077, 2426, 187, 2053, 1498, 1332, 4700, 7, 2866, 5879, 4823, 990, 5754, 39, 1544, 4243, 436, 790, 4657, 3795, 232, 4146, 5040, 167, 3322, 1546, 1070, 4455, 4535, 2229, 503, 2554, 1027, 2475, 1485, 937, 5119, 3026, 38, 4265, 105, 951, 220, 3835, 3973, 1972, 2746, 2484, 3040, 1298, 1729, 5053, 3178, 683, 938, 2593, 4633, 2234, 5402, 301, 4539, 4514, 3520, 3384, 5793, 3379, 243, 2670, 5071, 1144, 1591, 1496, 5610, 2073, 1726, 2062, 5811, 2702, 4886, 4084, 4463, 426, 1864, 557, 4395, 1353, 3015, 621, 5744, 2533, 2743, 3418, 854, 5848, 5345, 5144, 4137, 855, 2363, 2614, 1315, 4553, 2632, 2453, 3870, 1310, 2235, 3146, 835, 5444, 354, 696, 4462, 3533, 336, 3177, 3213, 5672, 5319, 67, 1350, 3596, 1087, 699, 5545, 1379, 896, 2622, 4441, 968, 2998, 5360, 1289, 341, 4102, 4576, 2865, 3679, 3380, 3119, 252, 2131, 2118, 5814, 5988, 1394, 3590, 3075, 3051, 4659, 123, 3995, 4114, 1558, 2894, 4320, 2848, 4862, 2048, 4408, 3388, 3151, 933, 3361, 1720, 2130, 3, 4493, 1266, 1160, 227, 2410, 3933, 4240, 2327, 2502, 5776, 2752, 593, 1466, 2830, 431, 5821, 5601, 3183, 732, 5153, 1555, 594, 1630, 199, 5539, 953, 5806, 1277, 1658, 4667, 5233, 650, 693, 4575, 1907, 4738, 2791, 2156, 970, 2113, 875, 1143, 5626, 946, 3314, 3511, 5469, 3368, 3513, 5250, 344, 5128, 3892, 5010, 1769, 2211, 4567, 1166, 680, 3763, 1829, 4555, 5471, 3340, 3904, 714, 945, 328, 5257, 3768, 3894, 842, 5782, 4393, 2272, 4296, 5304, 3551, 3642, 661, 5613, 2967, 5540, 15, 5013, 4444, 389, 188, 2242, 4688, 5218, 4355, 4139, 679, 2818, 1687, 5901, 192, 2762, 2419, 4599, 5055, 4410, 3557, 886, 4537, 2428, 5675, 1, 3222, 2724, 2940, 3158, 491, 5710, 4616, 5342, 2112, 4267, 5959, 5585, 3063, 5325, 4390, 4314, 4916, 5712, 371, 1054, 1736, 4173, 3274, 393, 125, 57, 3282, 4466, 1404, 182, 2546, 4049, 3311, 3952, 3773, 3446, 2626, 239, 2325, 2336, 787, 1044, 837, 1219, 5094, 1612, 5363, 2607, 5420, 3137, 5097, 2476, 1995, 2509, 3390, 620, 5840, 5036, 4200, 2255, 4312, 2132, 206, 1062, 483, 5424, 569, 1963, 2123, 5248, 4299, 1101, 4289, 1180, 5475, 4435, 5456, 4578, 1639, 191, 5805, 1453, 274, 2693, 4714, 3548, 2892, 4628, 3326, 2727, 2790, 2887, 4571, 4798, 5214, 4726, 445, 1646, 1275, 3649, 2976, 3108, 1184, 5992, 5726, 5753, 2802, 1447, 1767, 2103, 1133, 1271, 4365, 4772, 5027, 4804, 5222, 5662, 3489, 2480, 5570, 3537, 272, 4741, 2057, 1244, 2878, 3571, 3350, 4825, 2375, 3465, 2312, 5147, 5112, 3342, 3615, 1482, 921, 5909, 2276, 2416, 3302, 5880, 652, 5412, 4406, 4740, 2485, 180, 2527, 3211, 3308, 4005, 1600, 2811, 3535, 4871, 1417, 277, 4770, 3412, 5697, 598, 5488, 3792, 3975, 1038, 5203, 655, 3156, 717, 827, 4696, 2443, 1173, 4541, 2311, 1111, 3589, 751, 5954, 1678, 154, 3946, 3839, 2594, 2667, 4409, 5313, 2158, 3179, 4155, 169, 2270, 4165, 998, 216, 2498, 3811, 5569, 871, 3022, 3761, 5305, 1871, 3606, 2919, 3400, 3285, 3052, 2330, 5023, 4004, 4249, 4666, 4523, 89, 943, 1898, 5327, 5767, 1155, 516, 722, 2136, 5165, 1030, 2324, 4391, 1331, 1716, 4294, 4678, 5941, 2322, 435, 864, 4236, 3884, 1881, 1764, 5942, 3640, 249, 2797, 2023, 5666, 433, 1607, 130, 1286, 2814, 5, 5931, 1684, 5385, 994, 1534, 5018, 1217, 4840, 3560, 660, 4351, 1937, 4977, 1925, 4223, 5560, 2739, 2997, 5192, 4032, 3748, 5001, 1476, 4422, 5852, 5108, 4632, 5856, 2608, 4483, 4264, 2902, 5841, 5786, 5715, 956, 3142, 4068, 2098, 1073, 4938, 4387, 1886, 5621, 112, 5269, 4686, 1787, 641, 3087, 4912, 5032, 108, 2851, 3079, 4991, 3272, 443, 1858, 3048, 1304, 2284, 3628, 2216, 3848, 5881, 1838, 1899, 852, 1399, 2946, 5708, 2690, 2824, 4836, 5922, 2912, 1196, 5566, 3134, 4789, 1917, 742, 1887, 3002, 3702, 5914, 5049, 5725, 5958, 3248, 4163, 5655, 4544, 992, 2404, 2971, 5685, 5219, 773, 4923, 13, 3661, 2680, 5661, 3612, 4561, 3136, 874, 5734, 839, 5400, 2301, 3812, 3276, 5271, 4242, 3516, 1308, 3217, 5130, 174, 3720, 5289, 2429, 4974, 895, 2728, 2618, 3011, 1454, 289, 1927, 1798, 3826, 1842, 3752, 5835, 1464, 1041, 4874, 2321, 74, 643, 4689, 915, 93, 1677, 667, 1401, 3023, 5983, 4692, 3099, 2780, 4060, 3724, 638, 2841, 3565, 3049, 1348, 2344, 2060, 2146, 2198, 753, 1141, 4323, 5366, 1956, 5829, 1797, 4855, 513, 5226, 5557, 4430, 2249, 5034, 3503, 1444, 1837, 4099, 4519, 2377, 5689, 2012, 2115, 4228, 4830, 5228, 3247, 2469, 5985, 5803, 2165, 1468, 4007, 3391, 700, 489, 3515, 4154, 4384, 1597, 1757, 2709, 5202, 3569, 3895, 4841, 744, 1603, 1425, 3898, 183, 5473, 5106, 5361, 2439, 5924, 5174, 5215, 4983, 4058, 1588, 3587, 2807, 5962, 2462, 5682, 1556, 103, 5587, 4328, 2063, 3266, 4890, 2740, 5137, 4782, 4083, 3107, 5080, 5755, 28, 1581, 4278, 2494, 3804, 2959, 555, 2366, 2730, 5382, 1395, 41, 5150, 4506, 4990, 5206, 4512, 1574, 3041, 2425, 5006, 2861, 5912, 1620, 2708, 1071, 4185, 2597, 455, 4747, 5100, 1571, 474, 2833, 1287, 2413, 2150, 3092, 1316, 4913, 1328, 4513, 4050, 3463, 1696, 3719, 3988, 4842, 1869, 2460, 5925, 1713, 53, 3680, 1416, 5126, 1664, 5636, 4765, 2569, 3582, 5220, 654, 626, 175, 340, 5850, 2371, 5883, 599, 2975, 3977, 1683, 1876, 1589, 3444, 656, 3323, 1711, 4260, 3056, 3802, 2699, 3242, 1179, 4174, 1195, 2965, 387, 5991, 4363, 3972, 3392, 5834, 3486, 2813, 1863, 3913, 2615, 664, 5875, 2467, 3089, 3341, 3803, 4453, 4269, 651, 5333, 613, 3184, 4160, 2623, 4404, 5748, 5509, 279, 1590, 4382, 4438, 4676, 2753, 2217, 2034, 1458, 3103, 4697, 5737, 4071, 1375, 2872, 4662, 1405, 4497, 5284, 2718, 2455, 5139, 1817, 612, 2392, 731, 2931, 2560, 221, 352, 605, 1318, 1374, 1536, 1547, 1671, 1776, 2240, 2481, 2660, 3070, 3473, 3651, 3869, 3890, 4096, 4302, 5017, 5283, 5600, 5606, 5863, 1561, 2061, 1255, 1249, 2393, 3297, 778, 3666, 4241, 1490, 4105, 5679, 1487, 5780, 447, 2114, 3148, 5225, 3510, 386, 609, 560, 4613, 4307, 3317, 4431, 2968, 5801, 1472, 519, 1629, 2335, 3758, 4924, 692, 3949, 570, 510, 2635, 2654, 278, 3251, 2799, 1431, 5338, 4448, 882, 1977, 3760, 579, 4413, 3216, 4957, 4883, 5533, 5346, 5749, 149, 5997, 552, 4795, 798, 5652, 3981, 5073, 18, 1666, 3385, 1108, 2273, 4203, 1637, 2577, 3662, 463, 4531, 4684, 1924, 2490, 3676, 1161, 1584, 1437, 5818, 5335, 1174, 4076, 4326, 4169, 2920, 1032, 3097, 273, 891, 1474, 3339, 3249, 961, 5459, 4180, 2212, 3050, 1016, 4808, 1313, 1598, 2440, 3027, 4067, 4465, 4791, 5035, 5889, 1982, 2355, 865, 138, 4811, 772, 4024, 805, 5972, 2107, 5624, 2088, 3315, 847, 4473, 5692, 4476, 2254, 2342, 2776, 5735, 2678, 1457, 2603, 1075, 665, 928, 1305, 2058, 2300, 4665, 5117, 3963, 747, 1552, 5461, 1242, 2575, 1234, 2853, 2225, 3996, 3944, 4529, 3528, 2847, 4029, 851, 1175, 3372, 2219, 5076, 2162, 5565, 3830, 2564, 5381, 3566, 4377, 3162, 677, 2568, 1980, 432, 2434, 657, 4847, 518, 1197, 3469, 2508, 3638, 1414, 4617, 5217, 2873, 280, 4181, 469, 5223, 2697, 3153, 4546, 944, 1190, 5536, 3210, 4781, 325, 4572, 4609, 4771, 5038, 5470, 5986, 2555, 4583, 4713, 3353, 558, 653, 3201, 3728, 2368, 2089, 3731, 2054, 441, 1511, 959, 2074, 61, 64, 153, 423, 509, 561, 628, 688, 830, 1206, 1381, 1411, 1577, 1679, 1737, 1760, 1796, 1862, 1905, 1906, 1946, 1967, 2096, 2185, 2202, 2274, 2358, 2402, 2405, 2497, 2557, 2580, 2726, 2756, 3189, 3425, 3440, 3475, 3538, 3780, 3883, 3936, 4144, 4244, 4534, 4606, 4722, 4744, 4774, 4884, 4911, 4970, 5167, 5224, 5285, 5292, 5529, 5645, 5815, 294, 3741, 1573, 2906, 70, 1061, 3950, 5227, 5088, 1282, 3478, 4560, 5632, 3862, 3755, 2483, 2075, 250, 3682, 2947, 66, 986, 2224, 4690, 3772, 283, 2317, 2261, 4946, 5084, 5531, 2662, 327, 1539, 2891, 1168, 976, 1601, 3579, 5413, 3164, 3232, 5876, 2584, 899, 669, 3152, 2203, 1475, 1981, 2881, 1369, 562, 348, 2420, 5062, 129, 4141, 2996, 2364, 5525, 3244, 2705, 4640, 4143, 5913, 164, 955, 997, 3684, 4386, 1709, 5608, 1879, 5002, 5492, 144, 235, 3055, 3307, 1521, 1314, 511, 3926, 3671, 5860, 468, 2538, 5974, 5868, 2905, 4411, 1278, 4295, 3767, 5633, 2445, 1553, 1656, 1456, 2108, 3706, 4346, 5133, 4638, 2349, 663, 2133, 553, 2152, 4085, 3365, 62, 2357, 800, 3453, 1202, 5575, 314, 5021, 2768, 3327, 2145, 1491, 2218, 2741, 4559, 4863, 4702, 3042, 532, 1513, 5455, 5486, 5131, 2092, 1288, 3806, 1572, 4284, 3468, 3711, 3205, 3345, 4508, 5310, 1118, 1563, 5251, 996, 3495, 5422, 1717, 1791, 2195, 2775, 3098, 4607, 4325, 3821, 5542, 4587, 2033, 4252, 1894, 4850, 5179, 446, 572, 1739, 1033, 4502, 3902, 2511, 1947, 1136, 5950, 2398, 3157, 3858, 1579, 2397, 4710, 76, 1227, 2837, 312, 5442, 35, 940, 1756, 4653, 1779, 3004, 3069, 3709, 3683, 5849, 3507, 3954, 2520, 1068, 4136, 65, 5828, 3349, 2733, 5321, 4711, 2649, 2991, 3301, 1022, 3951, 4846, 5478, 4281, 2444, 3117, 5751, 1866, 2353, 4897, 2086, 1642, 2686, 3190, 5282, 4858, 4745, 2979, 31, 4733, 5908, 5975, 5329, 5946, 5134, 2388, 3292, 4699, 1080, 4110, 498, 4851, 2901, 4197, 4975, 1609, 2821, 258, 1850, 3017, 5176, 1251, 5770, 2962, 2711, 4369, 4834, 3960, 4381, 1714, 771, 1063, 4182, 3817, 5625, 1868, 5921, 5362, 5208, 1890, 4405, 5686, 5906, 1269, 3064, 2961, 4681, 1836, 5105, 5275, 3094, 2994, 357, 2072, 3787, 3123, 1772, 3595, 4768, 4128, 4794, 4149, 5647, 40, 4001, 5994, 4354, 3194, 4318, 5230, 1058, 2974, 2408, 1154, 317, 4605, 720, 4364, 4954, 1329, 2910, 1627, 1493, 2307, 5846, 3061, 2579, 122, 1471, 3395, 4204, 2155, 1309, 2701, 5303, 2264, 3283, 4797, 157, 1952, 5703, 4914, 4388, 5799, 711, 3718, 1051, 1667, 5210, 1843, 37, 1549, 528, 374, 5896, 707, 5784, 5373, 5838, 5670, 425, 6, 2815, 3820, 2700, 2642, 1810, 3223, 2871, 2715, 1364, 4270, 4769, 3866, 3038, 5497, 5390, 2308, 5654, 3938, 1430, 3928, 3072, 147, 1507, 2826, 3037, 3088, 3423, 3953, 5920, 769, 482, 55, 5681, 2738, 1619, 1100, 3066, 1322, 658, 4245, 884, 671, 5072, 4034, 5902, 5487, 5583, 5522, 1351, 3805, 4515, 211, 4305, 484, 807, 1615, 3147, 3338, 3586, 4093, 930, 3982, 4080, 3159, 5939, 659, 4259, 4611, 2536, 581, 5129, 1699, 3688, 4374, 762, 5855, 3377, 1017, 456, 2465, 5553, 3647, 3610, 3639, 4103, 4127, 4565, 9, 4235, 1042, 630, 5341, 3730, 202, 4135, 1056, 1012, 4111, 4225, 5404, 1638, 5434, 5641, 4824, 5387, 5899, 5809, 4330, 782, 3115, 171, 2134, 640, 5339, 5151, 2287, 3691, 4777, 179, 4367, 45, 5330, 2386, 3701, 4088, 4779, 1215, 4168, 1021, 2688, 107, 4951, 627, 3948, 83, 1245, 1672, 2582, 5665, 5318, 533, 372, 5510, 5200, 1335, 142, 361, 410, 1084, 1515, 1568, 1721, 1790, 3410, 3874, 4226, 4521, 5185, 5411, 5464, 5819, 4440, 2870, 4274, 1090, 1247, 3106, 983, 1727, 5255, 2180, 494, 1911, 1872, 2239, 3652, 1153, 797, 838, 4642, 2759, 1223, 5000, 226, 623, 783, 888, 1124, 1211, 1250, 1274, 1535, 1662, 1704, 1914, 2021, 2233, 2721, 2763, 3039, 3204, 3236, 3837, 3917, 4123, 4343, 4687, 5099, 5109, 5266, 5364, 5467, 5800, 780, 4290, 5739, 2382, 3774, 989, 5635, 1875, 963, 885, 1901, 3253, 724, 3801, 17, 3539, 1636, 2193, 804, 4903, 5746, 892, 2175, 1738, 1777, 4048, 4313, 2782, 287, 2070, 2389, 2895, 3230, 5009, 5398, 5823, 2401, 1354, 4399, 5733, 1986, 2819, 5050, 3471, 745, 4337, 5490, 2262, 3109, 5389, 2668, 4360, 1750, 5541, 3897, 2844, 3850, 3278, 5774, 586, 1820, 958, 437, 502, 1159, 4079, 5745, 1083, 729, 1941, 4300, 4670, 5312, 3759, 5656, 3261, 2778, 460, 4945, 3986, 4685, 4889, 5890, 3501, 4021, 564, 3939, 3143, 816, 2535, 5162, 4167, 3604, 2602, 2319, 580, 1884, 3219, 3525, 2036, 3605, 3255, 2222, 3740, 1613, 2619, 573, 36, 1824, 786, 3482, 591, 5195, 297, 1960, 3182, 5115, 3122, 5102, 5394, 2304, 4504, 418, 1723, 1420, 2849, 5201, 5414, 3288, 3438, 3062, 4929, 1356, 4547, 5426, 964, 5659, 4092, 4949, 763, 5598, 3485, 2250, 347, 1230, 1436, 3057, 2838, 2153, 1754, 1641, 1002, 3334, 5395, 1634, 5973, 2129, 1611, 4118, 3373, 5808, 3907, 5377, 238, 3029, 1280, 4785, 3653, 5302, 822, 2346, 4634, 803, 2896, 4885, 1938, 3167, 3568, 1324, 4543, 166, 1333, 160, 5827, 1582, 5207, 1334, 541, 1745, 4443, 2139, 3810, 1882, 1610, 4527, 2964, 102, 3404, 3448, 5631, 522, 2596, 2703, 1614, 5702, 587, 3352, 695, 5706, 3310, 5623, 2252, 5498, 873, 4170, 775, 378, 260, 3212, 3518, 1272, 3967, 1254, 3498, 1382, 3434, 3172, 2893, 4498, 3916, 2835, 3243, 2628, 4682, 1669, 1633, 3543, 132, 870, 1139, 113, 454, 743, 901, 1013, 1092, 2226, 2749, 2840, 3246, 3583, 4018, 4042, 4708, 4792, 4906, 5052, 5063, 5484, 5559, 5714, 5788, 5978, 927, 1398, 1680, 1915, 2059, 2359, 2473, 3316, 3488, 4814, 5173, 60, 863, 1393, 3846, 2617, 47, 214, 267, 453, 920, 1557, 1846, 2171, 2184, 3133, 3439, 3452, 3522, 3840, 5738, 1489, 2563, 3376, 4210, 209, 2127, 2687, 1239, 4981, 3754, 5246, 1834, 2448, 876, 2806, 1346, 10, 788, 4861, 2367, 4352, 540, 5259, 1762, 4845, 2571, 3573, 972, 4206, 5025, 5344, 2258, 1654, 3416, 4126, 4984, 4992, 5356, 5867, 1488, 5077, 1434, 4706, 3083, 1830, 1559, 5643, 2630, 4482, 2647, 2177, 1005, 4959, 848, 644, 1267, 2559, 231, 5061, 1391, 5156, 4793, 5270, 5765, 2822, 4349, 2332, 5688, 1992, 3318, 987, 5482, 3965, 4759, 2167, 5785, 5618, 5520, 1773, 3065, 4671, 5551, 290, 5489, 444, 5979, 46, 51, 52, 79, 163, 253, 291, 302, 316, 322, 380, 384, 385, 392, 421, 428, 448, 457, 467, 473, 490, 508, 549, 597, 614, 624, 631, 698, 710, 727, 748, 761, 792, 812, 814, 833, 834, 859, 879, 913, 936, 954, 978, 1069, 1099, 1130, 1198, 1201, 1218, 1235, 1248, 1257, 1261, 1337, 1344, 1413, 1426, 1483, 1492, 1501, 1532, 1594, 1623, 1651, 1778, 1802, 1892, 1951, 1974, 1975, 1976, 1983, 1991, 2014, 2032, 2042, 2052, 2083, 2160, 2310, 2329, 2374, 2409, 2427, 2447, 2458, 2514, 2524, 2550, 2572, 2609, 2611, 2644, 2659, 2665, 2710, 2779, 2829, 2852, 2862, 2904, 2917, 2949, 2984, 3021, 3028, 3046, 3114, 3149, 3206, 3229, 3237, 3257, 3279, 3305, 3369, 3398, 3470, 3508, 3517, 3523, 3540, 3580, 3591, 3608, 3613, 3629, 3636, 3686, 3687, 3727, 3742, 3776, 3781, 3860, 3861, 3885, 3934, 3966, 3984, 4014, 4025, 4051, 4078, 4156, 4195, 4211, 4248, 4275, 4279, 4321, 4334, 4394, 4432, 4442, 4516, 4538, 4618, 4619, 4844, 4868, 4899, 4904, 4930, 4943, 4953, 4955, 4966, 4985, 4987, 5007, 5042, 5074, 5161, 5196, 5240, 5252, 5311, 5351, 5365, 5384, 5410, 5433, 5451, 5462, 5480, 5505, 5591, 5671, 5676, 5691, 5717, 5741, 5742, 5797, 5810, 5817, 5898, 5948, 5980, 5989, 2010, 5098, 767, 4622, 305, 5221, 5397, 635, 4138, 2422, 4677, 5370, 2124, 2604, 808, 5057, 1530, 4879, 4673, 2757, 670, 1151, 948, 271, 115, 184, 931, 1523, 2285, 3313, 5622, 5743, 135, 307, 321, 346, 462, 701, 815, 817, 866, 932, 934, 1093, 1107, 1199, 1297, 1435, 1495, 1580, 1616, 1665, 1674, 1730, 1818, 1841, 1950, 2161, 2187, 2296, 2717, 2845, 2983, 2990, 3013, 3016, 3329, 3402, 3491, 3499, 3550, 3635, 3708, 3721, 3764, 3843, 3886, 4020, 4022, 4189, 4224, 4604, 4636, 4683, 4718, 4864, 5067, 5190, 5242, 5432, 5523, 5572, 5683, 5864, 5871, 5938, 5987, 3616, 101, 210, 2923, 4454, 736, 794, 4222, 4756, 5982, 177, 4445, 4778, 5349, 1486, 1948, 2981, 3362, 4134, 4937, 5615, 5707, 2102, 973, 1347, 1373, 4758, 5293, 784, 1749, 4621, 5491, 5149, 2750, 2732, 381, 2148, 3403, 893, 1813, 3231, 3992, 4980, 3868, 2008, 12, 3337, 539, 2210, 5026, 3355, 1128, 3713, 5260, 3698, 5630, 2793, 4843, 1074, 1903, 5033, 5421, 1855, 5095, 3575, 3074, 4233, 1519, 3700, 3786, 3450, 4615, 2747, 3169, 4964, 1978, 2142, 181, 1795, 869, 4477, 4736, 2674, 3399, 4998, 1299, 3675, 3526, 2664, 4176, 2645, 1545, 1094, 4915, 3490, 4035, 2471, 161, 3073, 1484, 2038, 168, 247, 2232, 2986, 3969, 4680, 4472, 914, 1439, 2748, 2842, 1077, 3945, 5907, 2186, 4288, 3738, 779, 2362, 4003, 1635, 4175, 1377, 5058, 5730, 2716, 4852, 3546, 465, 119, 5836, 1188, 5340, 2295, 5518, 524, 4996, 3779, 3937, 1345, 4164, 4562, 1294, 3426, 5253, 904, 3096, 1987, 809, 4184, 507, 4227, 1617, 4624, 5014, 3116, 373, 2856, 3744, 4637, 5204, 5406, 5790, 4549, 5966, 2548, 5332, 2545, 5337, 1256, 3306, 4494, 42, 5700, 4011, 1052, 2657, 3980, 2950, 1302, 5629, 3459, 5513, 1833, 1043, 5087, 495, 306, 1934, 5452, 916, 3033, 5166, 4031, 390, 1452, 133, 4072, 3909, 682, 616, 1809, 1327, 2011, 4648, 2646, 285, 1415, 466, 3857, 3563, 5732, 1117, 1127, 1912, 2029, 1700, 1784, 1020, 1823, 3544, 2858, 2168, 1200, 4183, 5199, 2570, 2121, 3110, 2850, 4488, 5142, 458, 4892, 4198, 5287, 2068, 5517, 2305, 4450, 3259, 5191, 2610, 2921, 5574, 5429, 126, 5163, 2784, 1954, 1273, 1171, 3911, 5851, 422, 1953, 567, 2846, 3785, 995, 5171, 3609, 5431, 5336, 5378, 1459, 350, 5660, 4276, 1761, 4107, 2140, 2085, 2534, 4287, 826, 4701, 5064, 4396, 5468, 1640, 2925, 4976, 5564, 1631, 4464, 5205, 3354, 324, 1499, 820, 5116, 5182, 4729, 1814, 1378, 2758, 947, 5447, 2631, 737, 704, 4282, 2190, 2306, 3545, 4763, 4672, 5441, 3145, 5844, 5760, 1004, 5934, 4385, 5231, 5713, 2205, 1578, 339, 4783, 2916, 1072, 4194, 212, 5507, 3008, 2683, 4507, 1421, 257, 409, 3429, 5696, 3514, 750, 514, 4120, 813, 4709, 1285, 4694, 2521, 4703, 1812, 5436, 5830, 3090, 54, 2213, 4033, 2221, 3309, 5211, 2181, 2966, 2884, 3968, 4728, 5887, 5937, 5773, 4451, 2056, 5315, 1701, 155, 2291, 577, 137, 2658, 5640, 590, 3809, 4838, 4641, 1477, 4860, 1001, 3324, 2326, 2999, 1522, 791, 4333, 2197, 1932, 303, 4647, 3295, 687, 5882, 5736, 2400, 2470, 4882, 3903, 4487, 5101, 5440, 5825, 2350, 1628, 918, 1748, 517, 201, 4199, 88, 4920, 2948, 3031, 1467, 1599, 3667, 3641, 1260, 1819, 2191, 1821, 3271, 3574, 4239, 1326, 3500, 2019, 3871, 3746, 1852, 957, 5331, 5628, 4, 19, 22, 23, 24, 25, 27, 32, 33, 34, 44, 49, 50, 59, 68, 69, 72, 73, 75, 84, 85, 86, 94, 95, 98, 100, 116, 124, 128, 134, 136, 141, 145, 146, 148, 150, 151, 165, 176, 185, 186, 189, 193, 197, 198, 200, 204, 207, 213, 215, 218, 219, 222, 224, 225, 233, 234, 237, 240, 241, 248, 256, 259, 261, 262, 264, 265, 268, 270, 275, 281, 282, 284, 286, 288, 292, 296, 298, 299, 309, 311, 329, 330, 331, 333, 337, 342, 349, 353, 355, 358, 359, 360, 369, 370, 375, 376, 377, 382, 391, 394, 395, 402, 403, 404, 407, 408, 413, 414, 415, 417, 419, 427, 430, 434, 442, 449, 450, 451, 459, 461, 471, 477, 478, 479, 485, 486, 487, 492, 496, 499, 504, 505, 515, 520, 521, 525, 529, 535, 536, 538, 545, 547, 551, 554, 556, 566, 571, 582, 583, 584, 588, 589, 592, 595, 596, 602, 603, 604, 606, 610, 615, 617, 618, 619, 622, 625, 634, 636, 637, 639, 645, 646, 648, 649, 666, 672, 673, 674, 675, 678, 685, 686, 689, 691, 694, 702, 703, 705, 708, 713, 715, 716, 718, 719, 723, 725, 741, 749, 754, 755, 759, 766, 768, 774, 781, 789, 793, 799, 806, 818, 824, 825, 829, 831, 832, 840, 844, 845, 858, 860, 862, 872, 877, 878, 887, 890, 897, 898, 903, 905, 907, 919, 935, 942, 952, 960, 962, 965, 966, 969, 974, 979, 981, 985, 993, 999, 1000, 1007, 1009, 1011, 1014, 1015, 1018, 1019, 1025, 1028, 1029, 1034, 1036, 1037, 1039, 1048, 1049, 1059, 1065, 1066, 1076, 1078, 1079, 1081, 1082, 1085, 1086, 1088, 1089, 1096, 1097, 1098, 1103, 1106, 1113, 1116, 1119, 1121, 1122, 1123, 1131, 1132, 1135, 1137, 1138, 1140, 1142, 1147, 1148, 1150, 1152, 1157, 1158, 1163, 1165, 1167, 1169, 1170, 1172, 1176, 1178, 1181, 1183, 1185, 1187, 1189, 1191, 1192, 1207, 1214, 1220, 1222, 1224, 1225, 1229, 1231, 1233, 1236, 1241, 1243, 1252, 1258, 1262, 1264, 1265, 1270, 1276, 1279, 1281, 1283, 1284, 1291, 1296, 1300, 1303, 1306, 1307, 1319, 1320, 1325, 1338, 1339, 1341, 1342, 1343, 1355, 1357, 1358, 1361, 1362, 1363, 1365, 1370, 1371, 1372, 1376, 1380, 1383, 1390, 1396, 1400, 1402, 1408, 1409, 1410, 1418, 1422, 1424, 1427, 1432, 1433, 1440, 1441, 1443, 1445, 1448, 1449, 1450, 1451, 1455, 1460, 1463, 1470, 1473, 1478, 1479, 1481, 1497, 1505, 1506, 1514, 1518, 1520, 1524, 1527, 1528, 1529, 1531, 1538, 1548, 1551, 1560, 1565, 1566, 1567, 1569, 1570, 1576, 1583, 1586, 1587, 1593, 1595, 1596, 1602, 1605, 1608, 1618, 1624, 1625, 1632, 1643, 1645, 1648, 1649, 1650, 1652, 1655, 1659, 1660, 1661, 1673, 1676, 1681, 1686, 1688, 1691, 1692, 1694, 1695, 1702, 1703, 1705, 1707, 1708, 1710, 1712, 1724, 1725, 1728, 1731, 1732, 1733, 1740, 1741, 1744, 1746, 1747, 1751, 1753, 1755, 1758, 1763, 1766, 1770, 1771, 1775, 1780, 1782, 1786, 1789, 1792, 1799, 1801, 1803, 1807, 1811, 1816, 1822, 1826, 1832, 1839, 1849, 1851, 1853, 1856, 1857, 1861, 1873, 1874, 1877, 1878, 1883, 1885, 1888, 1895, 1897, 1904, 1918, 1919, 1920, 1921, 1922, 1923, 1926, 1928, 1929, 1935, 1936, 1939, 1940, 1942, 1943, 1944, 1945, 1949, 1958, 1962, 1964, 1965, 1968, 1970, 1971, 1973, 1984, 1996, 1999, 2002, 2003, 2004, 2013, 2015, 2016, 2022, 2024, 2025, 2028, 2030, 2031, 2040, 2041, 2044, 2046, 2047, 2049, 2050, 2051, 2064, 2069, 2071, 2079, 2080, 2082, 2084, 2087, 2090, 2091, 2093, 2095, 2097, 2099, 2100, 2104, 2110, 2116, 2119, 2135, 2137, 2138, 2141, 2149, 2157, 2159, 2163, 2164, 2166, 2169, 2174, 2176, 2182, 2188, 2194, 2200, 2204, 2206, 2207, 2220, 2227, 2228, 2230, 2243, 2245, 2266, 2268, 2277, 2279, 2283, 2286, 2288, 2289, 2290, 2292, 2297, 2299, 2302, 2314, 2318, 2323, 2328, 2333, 2337, 2340, 2341, 2343, 2345, 2352, 2370, 2372, 2378, 2379, 2381, 2384, 2385, 2391, 2394, 2396, 2399, 2407, 2411, 2415, 2423, 2437, 2449, 2450, 2451, 2463, 2466, 2468, 2472, 2474, 2477, 2478, 2486, 2487, 2488, 2492, 2495, 2496, 2500, 2501, 2503, 2504, 2505, 2506, 2510, 2512, 2513, 2516, 2517, 2518, 2519, 2522, 2525, 2529, 2531, 2537, 2539, 2540, 2541, 2543, 2547, 2549, 2551, 2552, 2553, 2556, 2558, 2565, 2566, 2567, 2586, 2588, 2590, 2591, 2595, 2601, 2612, 2613, 2616, 2620, 2621, 2634, 2639, 2640, 2641, 2650, 2655, 2656, 2661, 2666, 2669, 2672, 2673, 2679, 2684, 2689, 2692, 2695, 2696, 2704, 2707, 2714, 2719, 2720, 2723, 2725, 2731, 2734, 2735, 2745, 2760, 2761, 2764, 2766, 2769, 2771, 2773, 2774, 2777, 2781, 2786, 2789, 2795, 2800, 2804, 2816, 2820, 2825, 2834, 2839, 2860, 2863, 2867, 2869, 2875, 2876, 2877, 2879, 2883, 2886, 2889, 2897, 2898, 2900, 2903, 2908, 2918, 2922, 2927, 2932, 2935, 2938, 2942, 2943, 2945, 2951, 2953, 2956, 2957, 2958, 2969, 2970, 2973, 2980, 2987, 2989, 2992, 3000, 3003, 3006, 3007, 3010, 3012, 3014, 3019, 3020, 3024, 3025, 3030, 3032, 3036, 3043, 3058, 3059, 3060, 3071, 3077, 3082, 3084, 3085, 3091, 3093, 3100, 3101, 3102, 3105, 3112, 3113, 3118, 3124, 3125, 3127, 3130, 3131, 3132, 3135, 3139, 3155, 3161, 3163, 3168, 3171, 3174, 3180, 3181, 3185, 3186, 3187, 3191, 3193, 3197, 3198, 3199, 3200, 3202, 3207, 3209, 3215, 3218, 3226, 3233, 3234, 3239, 3245, 3250, 3256, 3258, 3263, 3265, 3268, 3269, 3270, 3273, 3275, 3277, 3280, 3284, 3286, 3289, 3291, 3293, 3294, 3298, 3299, 3300, 3303, 3304, 3312, 3319, 3330, 3332, 3343, 3346, 3351, 3358, 3363, 3366, 3367, 3371, 3374, 3375, 3382, 3383, 3386, 3389, 3393, 3394, 3396, 3397, 3407, 3409, 3413, 3415, 3420, 3421, 3424, 3428, 3431, 3432, 3433, 3435, 3441, 3447, 3449, 3455, 3457, 3458, 3461, 3462, 3464, 3474, 3476, 3479, 3480, 3483, 3484, 3492, 3493, 3494, 3504, 3506, 3509, 3524, 3527, 3530, 3532, 3534, 3536, 3541, 3542, 3547, 3549, 3552, 3553, 3556, 3558, 3561, 3562, 3564, 3576, 3581, 3584, 3585, 3588, 3593, 3597, 3598, 3599, 3600, 3607, 3614, 3617, 3618, 3619, 3622, 3625, 3627, 3630, 3631, 3634, 3645, 3650, 3657, 3658, 3659, 3665, 3668, 3669, 3673, 3674, 3677, 3681, 3689, 3692, 3693, 3694, 3696, 3697, 3704, 3705, 3707, 3710, 3714, 3716, 3717, 3723, 3726, 3729, 3733, 3734, 3737, 3739, 3743, 3745, 3749, 3751, 3756, 3757, 3766, 3769, 3775, 3778, 3782, 3783, 3784, 3788, 3789, 3793, 3796, 3797, 3799, 3807, 3808, 3814, 3815, 3818, 3819, 3823, 3824, 3828, 3829, 3833, 3836, 3838, 3841, 3847, 3851, 3852, 3853, 3855, 3856, 3859, 3867, 3875, 3876, 3877, 3878, 3879, 3881, 3882, 3887, 3888, 3893, 3896, 3899, 3901, 3905, 3910, 3914, 3915, 3919, 3920, 3932, 3935, 3943, 3955, 3956, 3957, 3958, 3959, 3961, 3974, 3978, 3979, 3987, 3990, 3994, 3998, 4006, 4010, 4015, 4016, 4019, 4023, 4038, 4039, 4040, 4041, 4043, 4045, 4046, 4047, 4052, 4054, 4057, 4059, 4062, 4063, 4064, 4069, 4073, 4074, 4077, 4086, 4090, 4091, 4100, 4104, 4106, 4109, 4113, 4115, 4117, 4131, 4132, 4147, 4148, 4150, 4151, 4152, 4153, 4157, 4158, 4161, 4162, 4166, 4171, 4178, 4179, 4187, 4188, 4190, 4193, 4196, 4205, 4213, 4218, 4220, 4230, 4232, 4238, 4251, 4255, 4257, 4261, 4266, 4271, 4273, 4283, 4285, 4291, 4293, 4297, 4298, 4301, 4306, 4308, 4315, 4319, 4322, 4327, 4331, 4332, 4336, 4339, 4340, 4341, 4345, 4347, 4356, 4357, 4359, 4362, 4366, 4371, 4372, 4380, 4389, 4392, 4397, 4398, 4400, 4403, 4412, 4414, 4416, 4417, 4418, 4419, 4421, 4423, 4424, 4425, 4429, 4434, 4437, 4446, 4447, 4457, 4459, 4460, 4469, 4474, 4479, 4480, 4484, 4485, 4486, 4492, 4505, 4510, 4511, 4517, 4518, 4522, 4528, 4530, 4532, 4533, 4545, 4548, 4550, 4551, 4554, 4556, 4557, 4563, 4566, 4568, 4569, 4573, 4579, 4580, 4581, 4585, 4586, 4588, 4592, 4593, 4596, 4597, 4603, 4610, 4612, 4620, 4626, 4627, 4631, 4635, 4639, 4643, 4652, 4654, 4660, 4664, 4668, 4674, 4679, 4691, 4695, 4698, 4704, 4707, 4712, 4715, 4716, 4730, 4734, 4739, 4746, 4749, 4751, 4752, 4753, 4754, 4760, 4761, 4764, 4766, 4773, 4775, 4780, 4784, 4786, 4787, 4788, 4790, 4796, 4799, 4800, 4801, 4806, 4807, 4812, 4813, 4817, 4819, 4827, 4829, 4833, 4837, 4853, 4857, 4867, 4869, 4891, 4893, 4894, 4895, 4896, 4898, 4905, 4908, 4909, 4917, 4918, 4921, 4926, 4927, 4931, 4934, 4936, 4939, 4940, 4942, 4947, 4948, 4950, 4952, 4958, 4965, 4967, 4968, 4972, 4979, 4982, 4986, 4989, 4993, 4997, 4999, 5004, 5008, 5012, 5016, 5029, 5030, 5037, 5041, 5044, 5045, 5046, 5048, 5051, 5054, 5059, 5065, 5066, 5068, 5069, 5070, 5075, 5079, 5091, 5092, 5093, 5096, 5110, 5111, 5113, 5122, 5123, 5125, 5135, 5136, 5140, 5145, 5146, 5152, 5157, 5159, 5164, 5168, 5175, 5177, 5178, 5180, 5184, 5186, 5187, 5188, 5193, 5194, 5197, 5209, 5216, 5235, 5243, 5244, 5247, 5254, 5256, 5258, 5262, 5264, 5265, 5268, 5273, 5274, 5278, 5279, 5290, 5296, 5297, 5298, 5306, 5307, 5309, 5316, 5317, 5322, 5324, 5326, 5328, 5343, 5348, 5350, 5355, 5357, 5358, 5359, 5368, 5369, 5372, 5374, 5379, 5380, 5383, 5388, 5391, 5405, 5409, 5416, 5425, 5430, 5437, 5438, 5439, 5443, 5445, 5448, 5453, 5454, 5457, 5458, 5463, 5466, 5474, 5476, 5477, 5483, 5485, 5494, 5496, 5501, 5502, 5503, 5506, 5508, 5512, 5519, 5526, 5527, 5528, 5530, 5532, 5535, 5537, 5538, 5547, 5549, 5555, 5556, 5558, 5561, 5563, 5567, 5571, 5573, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5589, 5593, 5599, 5602, 5603, 5604, 5605, 5607, 5609, 5611, 5616, 5637, 5638, 5648, 5649, 5650, 5651, 5653, 5658, 5663, 5667, 5669, 5673, 5674, 5678, 5680, 5690, 5693, 5695, 5699, 5701, 5705, 5711, 5718, 5719, 5720, 5723, 5728, 5729, 5731, 5740, 5750, 5752, 5756, 5761, 5762, 5764, 5772, 5775, 5777, 5779, 5781, 5787, 5789, 5791, 5792, 5794, 5796, 5798, 5802, 5807, 5816, 5833, 5837, 5842, 5843, 5847, 5853, 5857, 5858, 5861, 5862, 5865, 5869, 5872, 5874, 5884, 5885, 5886, 5888, 5891, 5892, 5894, 5895, 5897, 5900, 5903, 5904, 5905, 5910, 5915, 5916, 5919, 5923, 5926, 5927, 5928, 5932, 5936, 5940, 5943, 5945, 5951, 5952, 5955, 5960, 5961, 5967, 5969, 5971, 5984, 5990, 5996, 5999]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5124, 4119, 2421, ..., 5990, 5996, 5999])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PennGrader - DO NOT CHANGE\n",
        "grader.grade(test_case_id = 'test_q42_word_ranking', answer = target_word_ranking[:5])"
      ],
      "metadata": {
        "id": "CIkdMzFj5sIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d451f5-7468-43cf-8882-5ab279d58796"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Free-response Questions [10 points]\n",
        "\n",
        "In the ranking tasks, play with different vector representations and different similarity functions. **Does one combination appear to work better than another? Do any interesting patterns emerge?**\n",
        "\n",
        "Some patterns you could touch upon (you can pick 1 aspect to answer):\n",
        "* The fourth column of `will_play_text.csv` contains the name of the character who spoke each line. Using the methods described above, **which characters are most similar? Least similar? **\n",
        "* Shakespeare's plays are traditionally classified into [comedies, histories, and tragedies](https://en.wikipedia.org/wiki/Shakespeare%27s_plays). **Can you use these vector representations to cluster the plays?**\n",
        "* Do the vector representations of **[female characters](https://en.wikipedia.org/wiki/Category:Female_Shakespearean_characters)** differ distinguishably from **[male ones](https://en.wikipedia.org/wiki/Category:Male_Shakespearean_characters)**?"
      ],
      "metadata": {
        "id": "YzSMUBU-5say"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer question 1:**\n",
        "\n",
        "We can reasonably assume similar characters would speak similar sentences/similar words. "
      ],
      "metadata": {
        "id": "C93HdnScFJ5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuples, document_names, vocab = read_in_shakespeare()\n",
        "vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
        "\n",
        "def read_character_in_shakspeare():\n",
        "    '''\n",
        "    Construct a dictionary with character name as keys,\n",
        "    (vocab_size, 1) array as values, each array is filled by the term frequency that \n",
        "    appears in the lines spoken by the character.\n",
        "\n",
        "    Return: \n",
        "      char_term: the dictionary has the format described above.\n",
        "    '''\n",
        "\n",
        "    char_term={}\n",
        "    with open('will_play_text.csv') as f:\n",
        "      csv_reader = csv.reader(f, delimiter=';')\n",
        "      for row in csv_reader:\n",
        "        # collect character names as keys\n",
        "        ch_name = row[4]\n",
        "\n",
        "        # collect tokenized words spoken by the character \n",
        "        line = row[5]\n",
        "        line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
        "        line_tokens = [token.lower() for token in line_tokens]\n",
        "\n",
        "        # set value for each key as an vocab_size by 1 array\n",
        "        if ch_name not in char_term:\n",
        "          char_term[ch_name]=np.zeros((len(vocab), 1))\n",
        "        \n",
        "        # calculate tokens as values to fill the array\n",
        "        for token in line_tokens: \n",
        "          char_term[ch_name][vocab_to_id[token]]+=1\n",
        "\n",
        "      f.close()\n",
        "\n",
        "    return char_term"
      ],
      "metadata": {
        "id": "8DvWJWGOFJJc"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(c_term, sim_fn):\n",
        "  '''\n",
        "    Construct a dictionary with character pair as keys,\n",
        "    their similarity socre as values.\n",
        "\n",
        "    Input:\n",
        "    c_term: the dictionary containing character name as keys,\n",
        "    (vocab_size, 1) array as values, each array is filled by the term frequency that \n",
        "    appears in the lines spoken by the character.\n",
        "    sim_fn: the similarity function to calculate character pair's similarity score.\n",
        "\n",
        "    Return: \n",
        "      similairty: the dictionary has the format described above.\n",
        "    '''\n",
        "\n",
        "  # a list of all characters\n",
        "  character = list(c_term.keys()) \n",
        "  num_char = len(c_term)\n",
        "\n",
        "  # initialize the similarity dictionary\n",
        "  similarity = {}\n",
        "  \n",
        "  # iterate each character and calculate their similarity with the rest of characters\n",
        "  for i in range(num_char):\n",
        "      for j in range(i+1, num_char):\n",
        "        similarity[(character[i], character[j])] = sim_fn(c_term[character[i]], c_term[character[j]])\n",
        "  return similarity\n"
      ],
      "metadata": {
        "id": "vn-G9ozXqyVG"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
        "char_term = read_character_in_shakspeare()\n",
        "\n",
        "for sim_fn in sim_fns:\n",
        "    similarity_pair = compute_similarity(char_term, sim_fn)  \n",
        "\n",
        "    rank = sorted(similarity_pair, key=similarity_pair.get, reverse=True) \n",
        "\n",
        "    print('with %s, most similar pair %s, least similar pair %s' % (sim_fn.__qualname__, rank[0], rank[-1]))"
      ],
      "metadata": {
        "id": "q155bD-M5sFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f41609-8f20-4828-cb94-f5d5823ab283"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with compute_cosine_similarity, most similar pair ('Second Pirate', 'Outlaws'), least similar pair ('Outlaws', 'Mariner')\n",
            "with compute_jaccard_similarity, most similar pair ('Second Pirate', 'Outlaws'), least similar pair ('Outlaws', 'Mariner')\n",
            "with compute_dice_similarity, most similar pair ('Second Pirate', 'Outlaws'), least similar pair ('Outlaws', 'Mariner')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra credit [15 points]\n",
        "\n",
        "Quantifying the goodness of one vector space representation over another can be very difficult to do.  It might ultimately require testing how the different vector representations change the performance when used in a downstream task like question answering. A common way of quantifying the goodness of word vectors is to use them to compare the similarity of words with human similarity judgments, and then calculate the correlation of the two rankings.\n",
        "\n",
        "If you would like extra credit on this assignment, you can quantify the goodness of each of the different vector space models that you produced (for instance by varying the size of the context window, picking PPMI or tf-idf, and selecting among cosine, Jaccard, and Dice).  You can calculate their scores on the [SimLex999 data set](https://www.cl.cam.ac.uk/~fh295/simlex.html), and compute their correlation with human judgments using [Kendall's Tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient).\n",
        "\n",
        "- **Deliverables**:\n",
        "    - In the following cells, explain what **experiments** you ran, and which **settings** *(i.e. size of the context window, picking PPMI or tf-idf, and selecting among cosine, Jaccard, and Dice)* had the highest correlation with human judgments. \n",
        "\n",
        "- **Hint**:\n",
        "    - For SimLex999 dataset, you only need to use `word1`, `word2`, and `SimLex999` columns, where `SimLex999` column is a **similarity rating by human beings** (ranging from 0 to 10) between `word1` and `word2`.\n",
        "    - For Kendal's Tau, you can use check [this python implementation](https://www.geeksforgeeks.org/python-kendall-rank-correlation-coefficient/#) for reference.\n",
        "\n"
      ],
      "metadata": {
        "id": "pLJCSOm9v7NN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Settings**:\n",
        "\n",
        "\n",
        "1.   fixed context window: +/-2\n",
        "2.   PPMI\n",
        "3. try different similarity functions, including Cosine, Jaccard, and Dice\n",
        "\n",
        "**Goal**:\n",
        "\n",
        "Experiment which similarity function can generate a vector model that has the highest correlation with humann judgement\n",
        "\n"
      ],
      "metadata": {
        "id": "cUDdLFcCu_FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Settings:\n",
        "    context window: +/-2; \n",
        "    PPMI; \n",
        "    Cosine, Jaccard, and Dice\n",
        "  \n",
        "  Experiment which similarity function can generate a vector model that has the highest correlation with humann judgement\n",
        "  '''\n",
        "\n",
        "def read_simlex999():\n",
        "\t'''Reads in the SimLex999 dataset and processes it into a list of tuples.\n",
        "\n",
        "\tEach tuple consists of\n",
        "\ttuple[0]: word1\n",
        "\ttuple[1]: word2\n",
        "  tuple[2]: human similarity rating\n",
        "\n",
        "\tReturns:\n",
        "\t\ttuples: A list of tuples in the above format.\n",
        "\t'''\n",
        "\n",
        "\ttuples = []\n",
        "\n",
        "\twith open('SimLex-999.txt', 'r') as f:\n",
        "\t\tlines =  [line.strip() for line in f][1:]\n",
        "\t\tfor line in lines:\n",
        "\t\t\twords = line.split('\\t')\n",
        "\t\t\tword_1= words[0]\n",
        "\t\t\tword_2= words[1]\n",
        "\t\t\trating = words[3]\n",
        "\t\t\ttuples.append((word_1, word_2, rating))\n",
        "\treturn tuples"
      ],
      "metadata": {
        "id": "B4kfSPylxOeN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuples_std = read_simlex999()\n",
        "len(tuples_std) # expect 999"
      ],
      "metadata": {
        "id": "xhkFs-7yxOcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581af0f1-88d9-41bc-8143-47c8d01e87da"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up SimLex data\n",
        "_, _, vocab = read_in_shakespeare()\n",
        "\n",
        "std = {}\n",
        "for tuple in tuples_std:\n",
        "  if tuple[0] in vocab and tuple[1] in vocab:\n",
        "    ''' for overlapping vocab of SimLex and 'wii_play_text.tsv',\n",
        "    construct a dictionary with word pair as key, human rating as value\n",
        "    '''\n",
        "    std[(tuple[0], tuple[1])] = tuple[2]\n",
        "\n",
        "# update vocab to the overlapping vocab between two dataset \n",
        "updated_vocab = set()\n",
        "for tu in std.keys():\n",
        "  updated_vocab.add(tu[0])\n",
        "  updated_vocab.add(tu[1])\n",
        "updated_vocab = list(updated_vocab)"
      ],
      "metadata": {
        "id": "6FrinmUx5sDY"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(updated_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVhM3ZD6-FSI",
        "outputId": "a6183b29-99d8-4f49-ecb7-016db8eb1b54"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "724"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct SimLex term-context matrix\n",
        "n = len(updated_vocab)\n",
        "vocab_to_id = dict(zip(updated_vocab, range(0, len(updated_vocab))))\n",
        "std_matrix  = np.zeros([n,n])\n",
        "for key, value in std.items():\n",
        "  word, context = key\n",
        "  word_index = vocab_to_id[word]\n",
        "  c_index = vocab_to_id[context]\n",
        "  std_matrix[word_index, c_index] = value\n",
        "\n",
        "# adjust the similarity score when word and context are the same\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    if i == j:\n",
        "      std_matrix[i][j] = 1.0\n",
        "\n",
        "std_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh4VjJO88aRu",
        "outputId": "f0e7dfc9-92f0-486f-a6ef-517bdb443e7f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(724, 724)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PPMI matrix from 'wii_play_text.tsv'\n",
        "tuples, _, _ = read_in_shakespeare()\n",
        "term_context_matrix = create_term_context_matrix(tuples, updated_vocab, context_window_size = 2)\n",
        "PPMI_matrix = create_PPMI_matrix(term_context_matrix)\n",
        "\n",
        "PPMI_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DKnm7d0x7gw",
        "outputId": "7d9095e0-7b5d-4a95-cc74-f44707854a1a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(724, 724)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kendalltau\n",
        "\n",
        "def compare_with_std(std_matrix, ppmi_matrix, sim_fn):\n",
        "  '''\n",
        "  Compare targeted dataset's term_term similarity with term_term human rated similarity based on SimLex dataset.\n",
        "  \n",
        "  Input:\n",
        "    std_matrix: term_term similarity matrix made from SimLex dataset.\n",
        "    ppmi_matrix: term_term PPMI weighted matrix made from the dataset to be compared with.\n",
        "    sim_fn: similarity function used to calculate the similarity score for ppmi_matrix.\n",
        "  \n",
        "  Return:\n",
        "    Kendall's Tau Correlation between standard and targeted vectors.\n",
        "  '''\n",
        "  ken = 0\n",
        "  \n",
        "  for i in range(n):\n",
        "    # calculate similarity \n",
        "    target = ppmi_matrix[:, i]\n",
        "    std = std_matrix[:, i]\n",
        "    similarity = []\n",
        "    for j in range(n):\n",
        "      similarity.append(sim_fn(target, ppmi_matrix[:, j]))\n",
        "    similarity = np.asarray(similarity)\n",
        "    coff, _ = kendalltau(std, similarity)\n",
        "    ken += coff\n",
        "  \n",
        "  ken = ken/n\n",
        "\n",
        "  return ken\n"
      ],
      "metadata": {
        "id": "4tKgs9jIAfF5"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
        "for sim_fn in sim_fns:\n",
        "  ken = compare_with_std(std_matrix, PPMI_matrix, sim_fn)\n",
        "  print('with %s, the correlation socre is %f' % (sim_fn.__qualname__, ken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX-QTE4kHfWs",
        "outputId": "89d367c9-26c1-4770-91d6-1065e6b45e35"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with compute_cosine_similarity, the correlation socre is 0.041452\n",
            "with compute_jaccard_similarity, the correlation socre is 0.042262\n",
            "with compute_dice_similarity, the correlation socre is 0.042262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**:\n",
        "\n",
        "With a fixed context window = 2, using PPMI weighted matrix, Jaccard, and Dice genearte a slightly better word_word vector space comapred to Cosine for 'wii_play_text.tsv' dataset."
      ],
      "metadata": {
        "id": "gL4aBMckIqVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "### Congratulation on finishing your homework! Here are the deliverables you need to submit to GradeScope\n",
        "- This notebook and py file: rename to `homework4.ipynb` and `homework4.py`. You can download the notebook and py file by going to the top-left corner of this webpage, `File -> Download -> Download .ipynb/.py`"
      ],
      "metadata": {
        "id": "JS-WxZRm5xKr"
      }
    }
  ]
}